{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# find GPU device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'dataFiles'\n",
    "datasetSize = '100K'\n",
    "earthquakeSampleFraction = 0.25\n",
    "noiseSampleFraction = 0.25\n",
    "csvFileName = f'{dataDir}/filtered_earthquakeSampleFraction_{earthquakeSampleFraction}_noiseSampleFraction_{noiseSampleFraction}.csv'\n",
    "hdf5FileName = f'{dataDir}/filtered_earthquakeSampleFraction_{earthquakeSampleFraction}_noiseSampleFraction_{noiseSampleFraction}.hdf5'\n",
    "\n",
    "trainSetFraction = 0.8\n",
    "valSetFraction = 0.1\n",
    "testSetFraction = 1 - (trainSetFraction + valSetFraction) \n",
    "\n",
    "# read the csv file\n",
    "df_csv = pd.read_csv(csvFileName)\n",
    "\n",
    "# set trace name as index\n",
    "df_csv.set_index(['trace_name'], inplace=True)\n",
    "\n",
    "# keep rows with valid data\n",
    "df_csv.drop(df_csv[df_csv['p_arrival_sample'].isna()].index, inplace=True)\n",
    "\n",
    "# split the dataset into train, validation and test\n",
    "df_train, df_val, df_test = np.split(df_csv.sample(frac = 1), [int(trainSetFraction*len(df_csv)), int((trainSetFraction + valSetFraction)*len(df_csv))])\n",
    "\n",
    "# read the hdf5 file\n",
    "hdf5Data = h5py.File(hdf5FileName, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length = 40000\n",
      "Val set length = 5000\n",
      "Test set length = 5000\n"
     ]
    }
   ],
   "source": [
    "print ('Train set length =', len(df_train))\n",
    "print ('Val set length =', len(df_val))\n",
    "print ('Test set length =', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define other relevant dirNames\n",
    "\n",
    "# dir to dump plots\n",
    "plotImgDir = 'plotImages/cnn1dPArrivalRegression'\n",
    "os.system(f'mkdir -p {plotImgDir}')\n",
    "\n",
    "# directories to dump neuralNet params\n",
    "netParamsDirName = 'netParams/cnn1dPArrivalRegression'\n",
    "os.system(f'mkdir -p {netParamsDirName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, listIDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.listIDs = listIDs\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.listIDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.listIDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = hdf5Data.get('data/' + ID)\n",
    "        X = np.array(X)\n",
    "        X = X.T\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Net(\n",
      "  (conv1): Conv1d(3, 12, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(12, 12, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(12, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (poolDiv2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (poolDiv4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropOut): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=3000, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# define a 1D convolutional neural network\n",
    "\n",
    "dropOutProb = 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropOutProb = 0.5):\n",
    "        \n",
    "        # inherit base class\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 12, kernel_size=5, stride = 1, padding = 2)\n",
    "                \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels = 12, out_channels = 12, kernel_size=5, stride = 1, padding = 2)\n",
    "                \n",
    "        # layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels = 12, out_channels = 8, kernel_size=5, stride = 1, padding = 2)\n",
    "        \n",
    "        # use downsampling by 2 for 1st 2 layers, and then downsample by 4 for the 3rd\n",
    "        self.poolDiv2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.poolDiv4 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropOut = nn.Dropout(p = dropOutProb)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features = 375 * 8, out_features = 16)\n",
    "        self.fc2 = nn.Linear(in_features = 16, out_features = 16)\n",
    "        self.fc3 = nn.Linear(in_features = 16, out_features = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.poolDiv2(F.relu(self.conv1(x)))\n",
    "        x = self.poolDiv2(F.relu(self.conv2(x)))\n",
    "        x = self.poolDiv4(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.dropOut(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):        \n",
    "        size = x.size()[1:]\n",
    "        num_features = 1        \n",
    "        for s in size:\n",
    "            num_features *= s            \n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net(dropOutProb)\n",
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss criteria\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 1] loss: 126284399.958950, trainLoss: 126284424.000000, valLoss: 452132.656250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 2] loss: 450134.894050, trainLoss: 450131.812500, valLoss: 429140.531250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 3] loss: 422510.773300, trainLoss: 422514.906250, valLoss: 397073.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 4] loss: 387038.729700, trainLoss: 387041.593750, valLoss: 359463.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 5] loss: 345939.408150, trainLoss: 345942.187500, valLoss: 316393.843750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 6] loss: 301901.312000, trainLoss: 301901.625000, valLoss: 272433.312500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 7] loss: 257638.348325, trainLoss: 257637.656250, valLoss: 229499.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 8] loss: 215423.896950, trainLoss: 215427.703125, valLoss: 189211.531250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 9] loss: 176822.156125, trainLoss: 176819.421875, valLoss: 153531.328125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 10] loss: 142737.641487, trainLoss: 142738.000000, valLoss: 123059.453125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 11] loss: 113623.662913, trainLoss: 113621.742188, valLoss: 97005.570312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 12] loss: 89618.339675, trainLoss: 89619.062500, valLoss: 76200.367188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 13] loss: 70576.744075, trainLoss: 70576.859375, valLoss: 59773.515625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 14] loss: 56202.618369, trainLoss: 56202.601562, valLoss: 48200.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 15] loss: 46034.800700, trainLoss: 46034.136719, valLoss: 40519.843750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 16] loss: 39448.979897, trainLoss: 39449.367188, valLoss: 35983.367188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 17] loss: 35690.484675, trainLoss: 35689.871094, valLoss: 33660.589844\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 18] loss: 33904.074791, trainLoss: 33904.425781, valLoss: 32818.175781\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 19] loss: 33256.091194, trainLoss: 33255.671875, valLoss: 32660.453125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 20] loss: 33089.876984, trainLoss: 33089.824219, valLoss: 32676.427734\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 21] loss: 33064.424356, trainLoss: 33065.062500, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 22] loss: 33062.108137, trainLoss: 33063.316406, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 23] loss: 33061.538841, trainLoss: 33061.214844, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 24] loss: 33061.886197, trainLoss: 33061.863281, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 25] loss: 33063.924447, trainLoss: 33064.414062, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 26] loss: 33062.641422, trainLoss: 33062.617188, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 27] loss: 33062.702959, trainLoss: 33062.093750, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 28] loss: 33063.132556, trainLoss: 33062.574219, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 29] loss: 33062.646378, trainLoss: 33061.988281, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 30] loss: 33062.051513, trainLoss: 33062.410156, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 31] loss: 33062.358822, trainLoss: 33062.234375, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 32] loss: 33062.447884, trainLoss: 33062.714844, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 33] loss: 33062.449359, trainLoss: 33061.285156, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 34] loss: 33062.969759, trainLoss: 33063.472656, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 35] loss: 33063.555988, trainLoss: 33063.562500, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 36] loss: 33062.690822, trainLoss: 33063.406250, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 37] loss: 33062.409950, trainLoss: 33062.214844, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 38] loss: 33060.555038, trainLoss: 33060.761719, valLoss: 32727.080078\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 39] loss: 33062.762816, trainLoss: 33063.937500, valLoss: 32686.095703\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 1] loss: 501617.843761, trainLoss: 501619.656250, valLoss: 147599.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 2] loss: 98424.652431, trainLoss: 98426.515625, valLoss: 91494.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 3] loss: 58602.208003, trainLoss: 58601.898438, valLoss: 49800.320312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 4] loss: 49591.441028, trainLoss: 49591.910156, valLoss: 42962.796875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 5] loss: 120599.118639, trainLoss: 120597.960938, valLoss: 96289.414062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 6] loss: 59142.060619, trainLoss: 59142.000000, valLoss: 49537.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 7] loss: 46272.393211, trainLoss: 46273.417969, valLoss: 39769.933594\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 8] loss: 36432.165909, trainLoss: 36431.449219, valLoss: 28300.501953\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 9] loss: 22898.566849, trainLoss: 22898.554688, valLoss: 15488.078125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 10] loss: 14459.787295, trainLoss: 14459.960938, valLoss: 12667.390625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 11] loss: 12691.588416, trainLoss: 12691.517578, valLoss: 11106.599609\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 12] loss: 11314.385323, trainLoss: 11314.199219, valLoss: 42611.171875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 13] loss: 10890.619807, trainLoss: 10890.503906, valLoss: 10027.142578\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 14] loss: 10523.059058, trainLoss: 10523.484375, valLoss: 9715.770508\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 15] loss: 10083.808574, trainLoss: 10084.250977, valLoss: 9918.975586\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 16] loss: 10214.976632, trainLoss: 10214.986328, valLoss: 9264.415039\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 17] loss: 9638.872619, trainLoss: 9639.475586, valLoss: 8704.198242\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 18] loss: 9001.949454, trainLoss: 9001.978516, valLoss: 8782.324219\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 19] loss: 8468.793934, trainLoss: 8468.572266, valLoss: 6919.303711\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 20] loss: 7963.653938, trainLoss: 7963.998047, valLoss: 6318.413574\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 21] loss: 10483.350647, trainLoss: 10483.330078, valLoss: 6966.896484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 22] loss: 7659.874566, trainLoss: 7660.108398, valLoss: 7037.210938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 23] loss: 7032.658893, trainLoss: 7032.687988, valLoss: 6883.525391\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 24] loss: 21673.342507, trainLoss: 21673.232422, valLoss: 12956.986328\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 25] loss: 12896.670115, trainLoss: 12896.823242, valLoss: 11635.053711\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 26] loss: 11964.704102, trainLoss: 11964.103516, valLoss: 11362.260742\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 27] loss: 11534.123859, trainLoss: 11533.997070, valLoss: 10840.883789\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 28] loss: 11083.809438, trainLoss: 11084.042969, valLoss: 10670.361328\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 29] loss: 10715.759116, trainLoss: 10715.956055, valLoss: 10496.722656\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 30] loss: 10315.399924, trainLoss: 10315.820312, valLoss: 9991.163086\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 31] loss: 10063.788706, trainLoss: 10063.634766, valLoss: 9699.330078\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 32] loss: 9799.273805, trainLoss: 9799.278320, valLoss: 9699.629883\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 33] loss: 9607.349218, trainLoss: 9607.721680, valLoss: 9880.848633\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 34] loss: 9413.679082, trainLoss: 9413.642578, valLoss: 9312.107422\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 35] loss: 9223.976943, trainLoss: 9223.934570, valLoss: 8941.062500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 36] loss: 17075.571051, trainLoss: 17075.837891, valLoss: 8978.265625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 37] loss: 9151.169226, trainLoss: 9151.515625, valLoss: 7737.578125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 38] loss: 7089.542591, trainLoss: 7089.230957, valLoss: 6635.378418\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 39] loss: 9602.960828, trainLoss: 9603.038086, valLoss: 7924.851074\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 1] loss: 2322140.713400, trainLoss: 2322165.000000, valLoss: 455166.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 2] loss: 155833382.358950, trainLoss: 155833472.000000, valLoss: 461249.156250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 3] loss: 469682.917900, trainLoss: 469468.593750, valLoss: 459940.968750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 4] loss: 468879.016450, trainLoss: 469016.718750, valLoss: 458634.437500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 5] loss: 467909.312300, trainLoss: 467926.968750, valLoss: 458617.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 6] loss: 468108.083200, trainLoss: 468043.062500, valLoss: 457211.468750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 7] loss: 465881.893250, trainLoss: 465870.593750, valLoss: 455929.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 8] loss: 464287.718400, trainLoss: 464290.843750, valLoss: 454256.468750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 9] loss: 462693.654750, trainLoss: 462693.718750, valLoss: 452657.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 10] loss: 390539.354838, trainLoss: 390581.250000, valLoss: 138905.718750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 11] loss: 343041.045581, trainLoss: 343086.187500, valLoss: 127608.523438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 12] loss: 122359.614625, trainLoss: 122397.976562, valLoss: 111389.960938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 13] loss: 191854.527075, trainLoss: 191888.718750, valLoss: 101863.710938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 14] loss: 96857.759303, trainLoss: 96828.867188, valLoss: 87943.273438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 15] loss: 87367.038697, trainLoss: 87394.210938, valLoss: 80760.820312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 16] loss: 118371.358406, trainLoss: 118337.593750, valLoss: 86526.671875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 17] loss: 85099.675025, trainLoss: 85103.406250, valLoss: 78116.328125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 18] loss: 417881.829895, trainLoss: 417898.531250, valLoss: 73230.234375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 19] loss: 75066.830897, trainLoss: 75054.664062, valLoss: 68749.078125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 20] loss: 66493.099502, trainLoss: 66491.593750, valLoss: 57886.066406\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 21] loss: 396182495.167789, trainLoss: 396182112.000000, valLoss: 58459.441406\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 22] loss: 60180.355231, trainLoss: 60158.468750, valLoss: 55703.734375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 23] loss: 82370.427344, trainLoss: 82350.859375, valLoss: 51597.414062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 24] loss: 54882.671623, trainLoss: 54870.367188, valLoss: 47152.480469\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 25] loss: 51381.982573, trainLoss: 51368.308594, valLoss: 45169.406250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 26] loss: 109820.703435, trainLoss: 109813.312500, valLoss: 45904.761719\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 27] loss: 255940.129937, trainLoss: 255938.015625, valLoss: 41189.242188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 28] loss: 51777.539448, trainLoss: 51785.058594, valLoss: 45244.285156\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 29] loss: 563720.423531, trainLoss: 563734.875000, valLoss: 43600.800781\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 30] loss: 43499.113220, trainLoss: 43494.847656, valLoss: 34498.812500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 31] loss: 50607.484980, trainLoss: 50607.984375, valLoss: 29650.623047\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 32] loss: 100829.063452, trainLoss: 100830.414062, valLoss: 31892.789062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 33] loss: 32084.809135, trainLoss: 32086.882812, valLoss: 24359.861328\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 34] loss: 292491.961305, trainLoss: 292491.406250, valLoss: 24401.773438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 35] loss: 207914.975542, trainLoss: 207914.453125, valLoss: 26440.097656\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 36] loss: 26167.204810, trainLoss: 26166.949219, valLoss: 18493.544922\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 37] loss: 20347.595022, trainLoss: 20347.500000, valLoss: 15221.388672\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 38] loss: 25198.707748, trainLoss: 25199.082031, valLoss: 17506.833984\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 39] loss: 19437.473910, trainLoss: 19437.328125, valLoss: 15120.474609\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 40] loss: 17020.507049, trainLoss: 17020.591797, valLoss: 14663.730469\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 41] loss: 16490.967776, trainLoss: 16490.427734, valLoss: 18395.841797\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 42] loss: 22737.755609, trainLoss: 22738.091797, valLoss: 15238.173828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 43] loss: 17917.418264, trainLoss: 17917.425781, valLoss: 14652.732422\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 44] loss: 16329.980146, trainLoss: 16329.479492, valLoss: 13599.817383\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 45] loss: 17432.578138, trainLoss: 17432.892578, valLoss: 13472.873047\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 46] loss: 17859.878398, trainLoss: 17859.974609, valLoss: 13826.919922\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 47] loss: 15623.809535, trainLoss: 15623.754883, valLoss: 11934.632812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 48] loss: 17470866.205311, trainLoss: 17470882.000000, valLoss: 18469.621094\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 49] loss: 19923.665970, trainLoss: 19923.050781, valLoss: 12319.107422\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 50] loss: 16332.211697, trainLoss: 16331.674805, valLoss: 11960.695312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 51] loss: 16999.717178, trainLoss: 16999.691406, valLoss: 12574.502930\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 52] loss: 14256.780912, trainLoss: 14257.151367, valLoss: 10227.035156\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 53] loss: 16087.560312, trainLoss: 16088.546875, valLoss: 9891.186523\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 54] loss: 16365.895016, trainLoss: 16365.622070, valLoss: 12392.890625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 55] loss: 14536.907274, trainLoss: 14535.669922, valLoss: 9647.543945\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 56] loss: 15100.400756, trainLoss: 15100.747070, valLoss: 10642.651367\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 57] loss: 14200.632065, trainLoss: 14201.522461, valLoss: 12289.137695\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 58] loss: 13102.903204, trainLoss: 13101.766602, valLoss: 9576.023438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 59] loss: 12407.618591, trainLoss: 12407.921875, valLoss: 8798.012695\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 60] loss: 10413.194294, trainLoss: 10413.704102, valLoss: 7795.228027\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 61] loss: 10697.958180, trainLoss: 10697.587891, valLoss: 8927.569336\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 62] loss: 13153.692906, trainLoss: 13153.904297, valLoss: 9467.806641\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 63] loss: 14401.911941, trainLoss: 14402.375977, valLoss: 10464.808594\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 64] loss: 12957.172790, trainLoss: 12957.398438, valLoss: 8043.634766\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 65] loss: 11681.570550, trainLoss: 11681.916992, valLoss: 7701.495605\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 66] loss: 14710.724233, trainLoss: 14710.356445, valLoss: 8443.230469\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 67] loss: 10745.455237, trainLoss: 10745.364258, valLoss: 7114.453613\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 68] loss: 273211229.176827, trainLoss: 273211200.000000, valLoss: 20526.216797\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 1] loss: 9958827.276050, trainLoss: 9958985.000000, valLoss: 457242.343750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 2] loss: 19968111.894250, trainLoss: 19968102.000000, valLoss: 455152.062500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 3] loss: 2954886.990100, trainLoss: 2954755.250000, valLoss: 454064.468750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 4] loss: 943024.291800, trainLoss: 943023.875000, valLoss: 434897.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 5] loss: 834335.261000, trainLoss: 834335.125000, valLoss: 403485.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 6] loss: 861883.275200, trainLoss: 861885.750000, valLoss: 331769.406250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 7] loss: 404262.558150, trainLoss: 404261.250000, valLoss: 273566.468750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 8] loss: 284163.433100, trainLoss: 284163.437500, valLoss: 218081.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 9] loss: 214523.701412, trainLoss: 214526.140625, valLoss: 156939.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 10] loss: 404812.445287, trainLoss: 404813.093750, valLoss: 140271.734375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 11] loss: 168773.501525, trainLoss: 168773.953125, valLoss: 132668.671875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 12] loss: 6353056.999731, trainLoss: 6353060.000000, valLoss: 134110.312500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 13] loss: 239748.792450, trainLoss: 239747.750000, valLoss: 129020.835938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 14] loss: 239503.276662, trainLoss: 239504.781250, valLoss: 129525.914062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 15] loss: 145854.275131, trainLoss: 145855.765625, valLoss: 125999.484375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 16] loss: 144952.467425, trainLoss: 144953.828125, valLoss: 123396.476562\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 17] loss: 190513.884019, trainLoss: 190513.921875, valLoss: 119693.140625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 18] loss: 207383.662775, trainLoss: 207381.593750, valLoss: 112405.492188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 19] loss: 141611.134219, trainLoss: 141609.593750, valLoss: 108817.289062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 20] loss: 126849.800838, trainLoss: 126849.242188, valLoss: 105642.546875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 21] loss: 126669.072687, trainLoss: 126670.781250, valLoss: 102677.546875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 22] loss: 212759.944244, trainLoss: 212759.937500, valLoss: 107658.265625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 23] loss: 111246.192431, trainLoss: 111246.656250, valLoss: 99146.031250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 24] loss: 100310.767331, trainLoss: 100310.343750, valLoss: 95595.742188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 25] loss: 613926.436756, trainLoss: 613922.875000, valLoss: 94609.726562\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 26] loss: 101386.764938, trainLoss: 101387.898438, valLoss: 92403.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 27] loss: 95219.289269, trainLoss: 95218.445312, valLoss: 89668.109375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 28] loss: 87606.347644, trainLoss: 87605.421875, valLoss: 87259.570312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 29] loss: 86182.043959, trainLoss: 86181.750000, valLoss: 89596.437500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 30] loss: 82352.572691, trainLoss: 82350.968750, valLoss: 82800.398438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 31] loss: 80567.484563, trainLoss: 80567.390625, valLoss: 78828.289062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 32] loss: 95276.937681, trainLoss: 95274.976562, valLoss: 91010.695312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 33] loss: 114697.333131, trainLoss: 114695.039062, valLoss: 81798.132812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 34] loss: 77409.838169, trainLoss: 77411.351562, valLoss: 117492.054688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 35] loss: 82068.451375, trainLoss: 82068.085938, valLoss: 76503.265625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 36] loss: 73370.309025, trainLoss: 73370.617188, valLoss: 71226.671875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 37] loss: 66427.938044, trainLoss: 66426.843750, valLoss: 69368.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 38] loss: 62343.908844, trainLoss: 62343.320312, valLoss: 67748.117188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 39] loss: 57269.228025, trainLoss: 57269.507812, valLoss: 69070.796875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 40] loss: 56733.296734, trainLoss: 56733.457031, valLoss: 59568.105469\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 41] loss: 51848.449653, trainLoss: 51849.429688, valLoss: 58329.464844\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 42] loss: 54444.076872, trainLoss: 54443.992188, valLoss: 60791.742188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 43] loss: 50155.115297, trainLoss: 50154.628906, valLoss: 55327.167969\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 44] loss: 76882.774194, trainLoss: 76883.335938, valLoss: 94976.429688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 45] loss: 78618.553941, trainLoss: 78618.718750, valLoss: 64285.957031\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 46] loss: 57399.956675, trainLoss: 57399.781250, valLoss: 62719.851562\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 47] loss: 53205.913631, trainLoss: 53206.261719, valLoss: 57727.574219\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 48] loss: 47690.613259, trainLoss: 47690.578125, valLoss: 52114.214844\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 49] loss: 46040.122700, trainLoss: 46040.933594, valLoss: 48666.789062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 50] loss: 54989.686477, trainLoss: 54990.500000, valLoss: 53571.296875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 51] loss: 51737.840625, trainLoss: 51737.660156, valLoss: 50087.050781\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 52] loss: 42349.884075, trainLoss: 42350.597656, valLoss: 47067.117188\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 53] loss: 39545.792062, trainLoss: 39545.925781, valLoss: 45963.558594\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 54] loss: 36440.992391, trainLoss: 36441.320312, valLoss: 46470.261719\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 55] loss: 34482.627070, trainLoss: 34482.886719, valLoss: 33826.136719\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 56] loss: 34282.024247, trainLoss: 34282.625000, valLoss: 36929.832031\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 57] loss: 34540.342033, trainLoss: 34540.257812, valLoss: 39837.812500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 58] loss: 37317.363022, trainLoss: 37318.042969, valLoss: 64155.601562\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 59] loss: 34777.792727, trainLoss: 34777.625000, valLoss: 39042.113281\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 60] loss: 30457.902712, trainLoss: 30458.332031, valLoss: 32541.714844\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 61] loss: 30061.635709, trainLoss: 30061.880859, valLoss: 36606.871094\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 62] loss: 28403.855645, trainLoss: 28403.589844, valLoss: 31832.195312\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 63] loss: 31299.503575, trainLoss: 31299.269531, valLoss: 33193.304688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 64] loss: 29604.257964, trainLoss: 29605.062500, valLoss: 29684.044922\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 65] loss: 37418.196167, trainLoss: 37418.464844, valLoss: 28641.916016\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 66] loss: 26590783.964308, trainLoss: 26590766.000000, valLoss: 42088.902344\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 67] loss: 36674.794020, trainLoss: 36674.996094, valLoss: 45187.941406\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 68] loss: 60219.671111, trainLoss: 60218.957031, valLoss: 43121.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 69] loss: 34115.949102, trainLoss: 34115.484375, valLoss: 41193.914062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 70] loss: 28197.488473, trainLoss: 28196.859375, valLoss: 38416.156250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 71] loss: 26881.996253, trainLoss: 26882.335938, valLoss: 37056.039062\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 72] loss: 25657.060269, trainLoss: 25656.923828, valLoss: 37550.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 73] loss: 25096.021143, trainLoss: 25095.896484, valLoss: 31682.296875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 74] loss: 24292.141092, trainLoss: 24292.085938, valLoss: 35824.710938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 75] loss: 25420.261333, trainLoss: 25420.673828, valLoss: 31335.951172\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 76] loss: 23839.382209, trainLoss: 23839.216797, valLoss: 25909.628906\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 77] loss: 23515.529270, trainLoss: 23515.427734, valLoss: 26460.160156\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 78] loss: 22013.495163, trainLoss: 22013.626953, valLoss: 23407.021484\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 79] loss: 22547.630569, trainLoss: 22548.066406, valLoss: 26327.804688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 80] loss: 20918.481223, trainLoss: 20918.058594, valLoss: 26530.685547\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 81] loss: 20218.365819, trainLoss: 20218.488281, valLoss: 25035.726562\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 82] loss: 19732.341867, trainLoss: 19732.402344, valLoss: 26619.871094\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 83] loss: 18561.590345, trainLoss: 18561.677734, valLoss: 23664.109375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 84] loss: 18985.556979, trainLoss: 18984.941406, valLoss: 26559.771484\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 85] loss: 18734.436580, trainLoss: 18734.591797, valLoss: 25125.792969\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 86] loss: 18334.228662, trainLoss: 18334.152344, valLoss: 27652.779297\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 87] loss: 17787.847728, trainLoss: 17787.904297, valLoss: 21505.148438\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 88] loss: 20302.565732, trainLoss: 20302.281250, valLoss: 27358.697266\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 89] loss: 22316.582961, trainLoss: 22315.839844, valLoss: 30050.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 90] loss: 18434.246680, trainLoss: 18434.115234, valLoss: 22989.152344\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 91] loss: 16569.605852, trainLoss: 16569.585938, valLoss: 21925.222656\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 92] loss: 15509.043576, trainLoss: 15508.875000, valLoss: 25743.974609\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 93] loss: 15376.544005, trainLoss: 15376.463867, valLoss: 22791.101562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 94] loss: 15509.729688, trainLoss: 15509.577148, valLoss: 25469.337891\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 95] loss: 17450.410970, trainLoss: 17450.595703, valLoss: 29152.072266\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 96] loss: 19427.015091, trainLoss: 19427.228516, valLoss: 37573.847656\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 1] loss: 7521292170.281500, trainLoss: 7521345024.000000, valLoss: 461249.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 2] loss: 467773.989500, trainLoss: 467761.343750, valLoss: 456027.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 3] loss: 461326.466100, trainLoss: 461328.843750, valLoss: 448255.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 4] loss: 451929.448450, trainLoss: 451932.875000, valLoss: 436732.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 5] loss: 438997.115500, trainLoss: 439008.437500, valLoss: 422868.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 6] loss: 421827.714100, trainLoss: 421816.906250, valLoss: 403135.468750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 7] loss: 399760.876650, trainLoss: 399763.375000, valLoss: 378012.343750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 8] loss: 372413.384000, trainLoss: 372410.000000, valLoss: 349253.906250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 9] loss: 339952.531700, trainLoss: 339949.343750, valLoss: 314267.093750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 10] loss: 303359.027825, trainLoss: 303359.593750, valLoss: 277354.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 11] loss: 264376.657000, trainLoss: 264375.937500, valLoss: 237566.796875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 12] loss: 225108.941625, trainLoss: 225110.593750, valLoss: 199667.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 13] loss: 187544.400750, trainLoss: 187545.312500, valLoss: 164186.359375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 14] loss: 153217.858437, trainLoss: 153219.406250, valLoss: 132936.843750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 15] loss: 123092.935737, trainLoss: 123091.476562, valLoss: 105378.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 16] loss: 97662.339187, trainLoss: 97662.382812, valLoss: 83133.773438\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 17] loss: 77037.246425, trainLoss: 77039.835938, valLoss: 65298.910156\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 18] loss: 61056.113506, trainLoss: 61053.617188, valLoss: 52165.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 19] loss: 49392.097375, trainLoss: 49392.878906, valLoss: 42994.238281\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 20] loss: 41548.908091, trainLoss: 41547.957031, valLoss: 37373.082031\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 21] loss: 36832.500928, trainLoss: 36832.332031, valLoss: 34311.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 22] loss: 34404.782644, trainLoss: 34405.683594, valLoss: 33044.539062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 23] loss: 33416.335234, trainLoss: 33416.621094, valLoss: 32679.474609\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 24] loss: 33124.633806, trainLoss: 33124.480469, valLoss: 32663.103516\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 25] loss: 33068.169816, trainLoss: 33068.875000, valLoss: 32686.095703\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 26] loss: 33062.401869, trainLoss: 33062.378906, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 27] loss: 33062.393825, trainLoss: 33062.000000, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 28] loss: 33062.473234, trainLoss: 33063.023438, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 29] loss: 33062.214494, trainLoss: 33062.656250, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 30] loss: 33062.306556, trainLoss: 33063.589844, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 31] loss: 33062.732322, trainLoss: 33062.660156, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 32] loss: 33062.616369, trainLoss: 33062.324219, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 33] loss: 33063.064744, trainLoss: 33062.921875, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 34] loss: 33062.817644, trainLoss: 33062.597656, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 35] loss: 33062.692734, trainLoss: 33062.363281, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 36] loss: 33062.598006, trainLoss: 33062.855469, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 37] loss: 33062.324697, trainLoss: 33062.050781, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 38] loss: 33062.595578, trainLoss: 33062.117188, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 39] loss: 33062.570816, trainLoss: 33062.511719, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 40] loss: 33062.607466, trainLoss: 33063.023438, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 41] loss: 33062.736297, trainLoss: 33063.289062, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 42] loss: 33062.600153, trainLoss: 33063.125000, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 43] loss: 33062.816919, trainLoss: 33064.187500, valLoss: 32711.416016\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 1] loss: 496095.815837, trainLoss: 496106.281250, valLoss: 453428.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 2] loss: 242957.572475, trainLoss: 242945.500000, valLoss: 87505.843750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 3] loss: 86981.096869, trainLoss: 86985.289062, valLoss: 68693.367188\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 4] loss: 139178.038687, trainLoss: 139185.578125, valLoss: 446544.312500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 5] loss: 302217.408631, trainLoss: 302220.843750, valLoss: 71742.882812\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 6] loss: 410054.852850, trainLoss: 410048.187500, valLoss: 428522.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 7] loss: 432522.807350, trainLoss: 432523.812500, valLoss: 418117.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 8] loss: 422684.828850, trainLoss: 422689.718750, valLoss: 409247.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 9] loss: 414274.721000, trainLoss: 414276.437500, valLoss: 401919.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 10] loss: 406381.242550, trainLoss: 406385.468750, valLoss: 393460.781250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 11] loss: 398727.338600, trainLoss: 398715.218750, valLoss: 386288.718750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 12] loss: 391237.228350, trainLoss: 391234.656250, valLoss: 379188.781250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 13] loss: 383871.612550, trainLoss: 383875.687500, valLoss: 372160.687500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 14] loss: 376607.825900, trainLoss: 376617.093750, valLoss: 365204.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 15] loss: 369432.576800, trainLoss: 369435.968750, valLoss: 357180.281250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 16] loss: 362336.387750, trainLoss: 362327.593750, valLoss: 350380.312500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 17] loss: 355321.927750, trainLoss: 355315.968750, valLoss: 343652.218750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 18] loss: 348388.368850, trainLoss: 348386.343750, valLoss: 336996.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 19] loss: 341535.567250, trainLoss: 341537.312500, valLoss: 330412.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 20] loss: 334757.028500, trainLoss: 334760.750000, valLoss: 323900.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 21] loss: 328056.234275, trainLoss: 328065.062500, valLoss: 317460.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 22] loss: 321431.837500, trainLoss: 321439.187500, valLoss: 310037.812500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 1] loss: 1194719.634400, trainLoss: 1194715.375000, valLoss: 389050.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 2] loss: 2833512.881638, trainLoss: 2833546.500000, valLoss: 141873.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 3] loss: 31871278263.843575, trainLoss: 31871068160.000000, valLoss: 381406.593750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 4] loss: 359705.495400, trainLoss: 359849.593750, valLoss: 312091.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 5] loss: 270861.035300, trainLoss: 270990.718750, valLoss: 212204.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 6] loss: 206098.161950, trainLoss: 206186.796875, valLoss: 160882.484375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 7] loss: 162057.762125, trainLoss: 162058.187500, valLoss: 154212.828125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 8] loss: 22043799.931637, trainLoss: 22043702.000000, valLoss: 148752.328125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 9] loss: 148760.264444, trainLoss: 148668.046875, valLoss: 139610.593750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 10] loss: 146520.891944, trainLoss: 146458.609375, valLoss: 129631.328125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 11] loss: 1850291.030656, trainLoss: 1850236.875000, valLoss: 123054.664062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 12] loss: 134378.160844, trainLoss: 134338.140625, valLoss: 112675.031250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 13] loss: 1436317.623369, trainLoss: 1436296.000000, valLoss: 111821.734375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 14] loss: 112281.744419, trainLoss: 112286.695312, valLoss: 99408.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 15] loss: 112522.722919, trainLoss: 112538.390625, valLoss: 85204.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 16] loss: 2754530087.480837, trainLoss: 2754533632.000000, valLoss: 111108.718750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 17] loss: 114884.250200, trainLoss: 114928.968750, valLoss: 107659.226562\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 18] loss: 107524.029013, trainLoss: 107579.671875, valLoss: 97625.835938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 19] loss: 102959.537063, trainLoss: 103017.140625, valLoss: 95046.273438\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 20] loss: 122245.616391, trainLoss: 122184.226562, valLoss: 97694.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 21] loss: 100291.436406, trainLoss: 100248.781250, valLoss: 92337.421875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 22] loss: 98265.522669, trainLoss: 98253.648438, valLoss: 87141.015625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 23] loss: 3554254.880869, trainLoss: 3554256.750000, valLoss: 73337.148438\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 24] loss: 87353.512950, trainLoss: 87384.359375, valLoss: 74166.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 25] loss: 144989.157400, trainLoss: 144968.625000, valLoss: 78793.671875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 26] loss: 87329.456550, trainLoss: 87318.382812, valLoss: 75921.390625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 27] loss: 85336.244703, trainLoss: 85358.937500, valLoss: 72103.195312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 28] loss: 2511680.949622, trainLoss: 2511670.250000, valLoss: 111601.304688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 29] loss: 169012.720350, trainLoss: 169010.156250, valLoss: 70446.671875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 30] loss: 30929591.068097, trainLoss: 30929586.000000, valLoss: 1611501.000000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 1] loss: 1196476.035100, trainLoss: 1196504.375000, valLoss: 457632.656250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 2] loss: 4560234.007750, trainLoss: 4560332.500000, valLoss: 456829.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 3] loss: 595684.060450, trainLoss: 595698.250000, valLoss: 454005.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 4] loss: 4640385.709050, trainLoss: 4640368.000000, valLoss: 455052.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 5] loss: 2581953.211200, trainLoss: 2581934.000000, valLoss: 455350.093750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 6] loss: 952374.829050, trainLoss: 952377.750000, valLoss: 452154.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 7] loss: 545088.752150, trainLoss: 545087.750000, valLoss: 446889.968750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 8] loss: 467756.056850, trainLoss: 467754.281250, valLoss: 443493.812500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 9] loss: 451301.107950, trainLoss: 451302.125000, valLoss: 438419.093750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 10] loss: 793330.599700, trainLoss: 793331.187500, valLoss: 434909.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 11] loss: 438433.036800, trainLoss: 438432.656250, valLoss: 425321.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 12] loss: 429014.978400, trainLoss: 429013.593750, valLoss: 414103.031250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 13] loss: 411304.101600, trainLoss: 411306.750000, valLoss: 393889.281250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 14] loss: 394584.386700, trainLoss: 394585.031250, valLoss: 368150.406250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 15] loss: 357727.776750, trainLoss: 357729.937500, valLoss: 328710.531250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 16] loss: 305637.446950, trainLoss: 305638.843750, valLoss: 273950.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 17] loss: 242482.027975, trainLoss: 242480.640625, valLoss: 211534.296875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 18] loss: 175881.923750, trainLoss: 175882.765625, valLoss: 141736.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 19] loss: 122513.633450, trainLoss: 122513.804688, valLoss: 110140.453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 20] loss: 99697.699350, trainLoss: 99696.710938, valLoss: 107509.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 21] loss: 99025.072069, trainLoss: 99024.054688, valLoss: 101209.421875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 22] loss: 93756.838850, trainLoss: 93757.500000, valLoss: 100562.976562\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 23] loss: 88778.518538, trainLoss: 88777.570312, valLoss: 103079.132812\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 24] loss: 85980.646738, trainLoss: 85980.015625, valLoss: 88837.796875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 25] loss: 84715.120425, trainLoss: 84712.859375, valLoss: 88032.148438\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 26] loss: 104646.723400, trainLoss: 104646.679688, valLoss: 104367.695312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 27] loss: 103228.759919, trainLoss: 103228.648438, valLoss: 89693.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 28] loss: 85899.970119, trainLoss: 85899.562500, valLoss: 91019.406250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 29] loss: 84917.929950, trainLoss: 84918.500000, valLoss: 84544.960938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 30] loss: 132912.502000, trainLoss: 132913.359375, valLoss: 100391.421875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 31] loss: 94387.253437, trainLoss: 94388.390625, valLoss: 95033.664062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 32] loss: 88996.686706, trainLoss: 88997.421875, valLoss: 88538.265625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 33] loss: 110161.708544, trainLoss: 110161.632812, valLoss: 89629.664062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 34] loss: 94231.178550, trainLoss: 94230.421875, valLoss: 83989.453125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 35] loss: 77592.709312, trainLoss: 77591.882812, valLoss: 81869.679688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 36] loss: 71169.605197, trainLoss: 71169.250000, valLoss: 82742.523438\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 37] loss: 70395.686631, trainLoss: 70396.000000, valLoss: 83359.765625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 38] loss: 67107.993175, trainLoss: 67107.789062, valLoss: 80065.335938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 39] loss: 61551.871597, trainLoss: 61551.417969, valLoss: 87487.796875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 40] loss: 60979.712063, trainLoss: 60978.667969, valLoss: 67964.632812\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 41] loss: 62331.981044, trainLoss: 62333.687500, valLoss: 68447.960938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 42] loss: 78669.217259, trainLoss: 78668.765625, valLoss: 79866.281250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 43] loss: 113852.325822, trainLoss: 113853.062500, valLoss: 61010.425781\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 44] loss: 55689.779478, trainLoss: 55690.851562, valLoss: 59432.601562\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 45] loss: 59859.685122, trainLoss: 59858.960938, valLoss: 58612.675781\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 46] loss: 52999.771025, trainLoss: 52999.144531, valLoss: 55281.316406\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 47] loss: 51223.532638, trainLoss: 51223.492188, valLoss: 54809.777344\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 48] loss: 65060.689538, trainLoss: 65060.406250, valLoss: 50287.574219\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 49] loss: 2248962.381694, trainLoss: 2248956.000000, valLoss: 60555.960938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 50] loss: 58380.173288, trainLoss: 58380.953125, valLoss: 54347.660156\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 51] loss: 61266.754047, trainLoss: 61266.425781, valLoss: 54172.304688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 52] loss: 57418.205253, trainLoss: 57417.468750, valLoss: 51940.027344\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 53] loss: 77827.331672, trainLoss: 77826.281250, valLoss: 49974.769531\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 54] loss: 52505.287594, trainLoss: 52505.816406, valLoss: 46138.664062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 55] loss: 53613.585575, trainLoss: 53613.769531, valLoss: 45368.304688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 56] loss: 49563.065391, trainLoss: 49563.414062, valLoss: 43703.285156\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 57] loss: 49232.194113, trainLoss: 49232.199219, valLoss: 45653.222656\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 58] loss: 45881.687312, trainLoss: 45882.816406, valLoss: 44692.851562\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 59] loss: 45483.886581, trainLoss: 45483.207031, valLoss: 47325.292969\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 60] loss: 44999.733122, trainLoss: 45000.371094, valLoss: 50051.402344\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 61] loss: 44040.502272, trainLoss: 44041.574219, valLoss: 40996.003906\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 62] loss: 43086.277834, trainLoss: 43086.320312, valLoss: 45352.554688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 63] loss: 42716.115569, trainLoss: 42716.117188, valLoss: 40016.710938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 64] loss: 57995.550319, trainLoss: 57997.062500, valLoss: 41747.785156\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 65] loss: 43390.362995, trainLoss: 43391.011719, valLoss: 36866.695312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 66] loss: 45266.520656, trainLoss: 45266.765625, valLoss: 36584.070312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 67] loss: 41837.738581, trainLoss: 41836.863281, valLoss: 38392.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 68] loss: 39447.467845, trainLoss: 39447.628906, valLoss: 39848.261719\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 69] loss: 38821.333287, trainLoss: 38821.726562, valLoss: 36135.925781\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 70] loss: 54602.725117, trainLoss: 54603.347656, valLoss: 35622.957031\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 71] loss: 41137.238037, trainLoss: 41137.597656, valLoss: 38146.679688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 72] loss: 39923.736269, trainLoss: 39924.324219, valLoss: 38434.214844\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 73] loss: 37747.173950, trainLoss: 37747.632812, valLoss: 37519.347656\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 74] loss: 35473.130031, trainLoss: 35473.132812, valLoss: 39409.417969\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 75] loss: 36076.789127, trainLoss: 36076.222656, valLoss: 33070.960938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 76] loss: 32983.074378, trainLoss: 32982.085938, valLoss: 35471.679688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 77] loss: 33602.515191, trainLoss: 33602.578125, valLoss: 34257.566406\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 78] loss: 35030.975156, trainLoss: 35030.062500, valLoss: 37162.945312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 79] loss: 33196.142867, trainLoss: 33196.832031, valLoss: 29596.101562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 80] loss: 32120.429037, trainLoss: 32120.683594, valLoss: 29288.011719\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 81] loss: 32594.927917, trainLoss: 32594.818359, valLoss: 36180.421875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 82] loss: 29977.533425, trainLoss: 29977.369141, valLoss: 30622.443359\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 83] loss: 28443.048228, trainLoss: 28443.320312, valLoss: 31911.355469\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 84] loss: 27039.996158, trainLoss: 27040.482422, valLoss: 33375.488281\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 85] loss: 26438.014178, trainLoss: 26437.548828, valLoss: 26897.955078\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 86] loss: 27673.562705, trainLoss: 27674.492188, valLoss: 29969.320312\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 87] loss: 26873.742336, trainLoss: 26874.777344, valLoss: 26817.011719\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 88] loss: 26379.131678, trainLoss: 26379.097656, valLoss: 35660.097656\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 89] loss: 27974.719677, trainLoss: 27974.328125, valLoss: 26010.691406\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 90] loss: 643189.836266, trainLoss: 643191.000000, valLoss: 27962.863281\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 91] loss: 25650.637280, trainLoss: 25651.421875, valLoss: 38826.394531\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 1] loss: 645322.333200, trainLoss: 645323.312500, valLoss: 388671.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 2] loss: 363391.524300, trainLoss: 363386.343750, valLoss: 322821.843750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 3] loss: 300704.851550, trainLoss: 300709.343750, valLoss: 265626.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 4] loss: 246433.479150, trainLoss: 246428.812500, valLoss: 215558.343750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 5] loss: 199798.027350, trainLoss: 199798.375000, valLoss: 173034.437500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 6] loss: 160156.793613, trainLoss: 160155.953125, valLoss: 137419.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 7] loss: 126973.100775, trainLoss: 126971.609375, valLoss: 108100.656250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 8] loss: 99798.047200, trainLoss: 99797.328125, valLoss: 84490.789062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 9] loss: 78186.937719, trainLoss: 78184.757812, valLoss: 66025.601562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 10] loss: 61650.341900, trainLoss: 61651.019531, valLoss: 52445.410156\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 11] loss: 49702.212794, trainLoss: 49702.257812, valLoss: 43198.566406\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 12] loss: 41709.728750, trainLoss: 41710.164062, valLoss: 37511.421875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 13] loss: 36900.151334, trainLoss: 36900.851562, valLoss: 34311.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 14] loss: 34434.392844, trainLoss: 34434.554688, valLoss: 33044.539062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 15] loss: 33427.825659, trainLoss: 33427.660156, valLoss: 32689.806641\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 16] loss: 33130.620872, trainLoss: 33130.417969, valLoss: 32663.103516\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 17] loss: 33069.751409, trainLoss: 33070.093750, valLoss: 32686.095703\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 18] loss: 33062.644122, trainLoss: 33063.976562, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 19] loss: 33062.526472, trainLoss: 33062.035156, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 20] loss: 33062.280997, trainLoss: 33062.820312, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 21] loss: 33062.230584, trainLoss: 33062.390625, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 22] loss: 33063.195394, trainLoss: 33063.769531, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 23] loss: 33062.557559, trainLoss: 33062.449219, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 24] loss: 33063.112047, trainLoss: 33061.988281, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 25] loss: 33061.001147, trainLoss: 33060.566406, valLoss: 32676.427734\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 26] loss: 33063.501888, trainLoss: 33063.511719, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 27] loss: 33062.637447, trainLoss: 33063.394531, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 28] loss: 33062.839700, trainLoss: 33063.277344, valLoss: 32686.095703\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 29] loss: 33062.430809, trainLoss: 33062.289062, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 30] loss: 33062.198603, trainLoss: 33062.867188, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 31] loss: 33063.067037, trainLoss: 33063.441406, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 32] loss: 33062.537375, trainLoss: 33062.046875, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 33] loss: 33062.580706, trainLoss: 33062.886719, valLoss: 32697.755859\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 34] loss: 33062.900981, trainLoss: 33063.062500, valLoss: 32711.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 35] loss: 33062.684481, trainLoss: 33062.367188, valLoss: 32711.416016\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 1] loss: 256564.126638, trainLoss: 256571.156250, valLoss: 32810.570312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 2] loss: 34868.106638, trainLoss: 34868.269531, valLoss: 32969.269531\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 3] loss: 34561.031928, trainLoss: 34561.574219, valLoss: 33761.082031\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 4] loss: 34735.785916, trainLoss: 34736.226562, valLoss: 32736.537109\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 5] loss: 34437.185441, trainLoss: 34437.437500, valLoss: 33442.023438\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 6] loss: 34237.268697, trainLoss: 34237.187500, valLoss: 32623.183594\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 7] loss: 34203.559316, trainLoss: 34203.230469, valLoss: 32196.523438\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 8] loss: 30814.014614, trainLoss: 30814.539062, valLoss: 15347.281250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 9] loss: 17430.855078, trainLoss: 17430.654297, valLoss: 14061.230469\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 10] loss: 15580.106490, trainLoss: 15579.494141, valLoss: 12027.178711\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 11] loss: 15265.850750, trainLoss: 15266.599609, valLoss: 12161.334961\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 12] loss: 14481.228461, trainLoss: 14481.106445, valLoss: 12162.057617\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 13] loss: 13680.094817, trainLoss: 13680.018555, valLoss: 11642.946289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 14] loss: 13571.686778, trainLoss: 13571.907227, valLoss: 10915.534180\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 15] loss: 12922.832682, trainLoss: 12922.818359, valLoss: 10546.878906\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 16] loss: 14681.527801, trainLoss: 14682.038086, valLoss: 15201.093750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 17] loss: 13779.176620, trainLoss: 13779.042969, valLoss: 10474.831055\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 18] loss: 13705.555340, trainLoss: 13705.083008, valLoss: 10923.876953\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 19] loss: 12531.146284, trainLoss: 12530.830078, valLoss: 10137.693359\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 20] loss: 12592.078398, trainLoss: 12592.112305, valLoss: 10407.869141\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 21] loss: 12023.717240, trainLoss: 12023.584961, valLoss: 9886.333984\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 22] loss: 24096.301296, trainLoss: 24095.802734, valLoss: 10628.088867\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 23] loss: 12623.812442, trainLoss: 12624.216797, valLoss: 10050.629883\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 24] loss: 12856.223202, trainLoss: 12855.754883, valLoss: 10426.659180\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 25] loss: 12248.862919, trainLoss: 12248.868164, valLoss: 10065.682617\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 26] loss: 12512.621609, trainLoss: 12512.955078, valLoss: 10041.548828\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 27] loss: 11751.073058, trainLoss: 11750.864258, valLoss: 9361.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 28] loss: 18005.797778, trainLoss: 18005.851562, valLoss: 10572.506836\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 29] loss: 12208.172800, trainLoss: 12208.158203, valLoss: 9684.544922\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 30] loss: 11668.895690, trainLoss: 11669.687500, valLoss: 10015.326172\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 31] loss: 11567.539898, trainLoss: 11567.912109, valLoss: 9824.930664\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 32] loss: 11292.411030, trainLoss: 11292.677734, valLoss: 10572.455078\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 33] loss: 11229.235912, trainLoss: 11229.222656, valLoss: 9476.959961\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 34] loss: 11415.872193, trainLoss: 11415.762695, valLoss: 9464.288086\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 35] loss: 25359.974984, trainLoss: 25360.185547, valLoss: 12400.416016\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 1] loss: 6297367.250600, trainLoss: 6297348.500000, valLoss: 448579.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 2] loss: 4947839.611900, trainLoss: 4947838.000000, valLoss: 176479.859375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 3] loss: 126250.120819, trainLoss: 126251.710938, valLoss: 85872.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 4] loss: 84111.070928, trainLoss: 84110.406250, valLoss: 60233.105469\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 5] loss: 62870.088094, trainLoss: 62871.007812, valLoss: 48415.839844\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 6] loss: 54171.359072, trainLoss: 54170.867188, valLoss: 42073.238281\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 7] loss: 74728.761228, trainLoss: 74728.937500, valLoss: 44199.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 8] loss: 47412.653863, trainLoss: 47413.851562, valLoss: 37667.378906\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 9] loss: 43151.100984, trainLoss: 43151.648438, valLoss: 33177.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 10] loss: 40496.611855, trainLoss: 40496.597656, valLoss: 30433.205078\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 11] loss: 37767.698408, trainLoss: 37767.914062, valLoss: 27661.416016\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 12] loss: 34722.999955, trainLoss: 34723.433594, valLoss: 25407.093750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 13] loss: 42781.425119, trainLoss: 42781.312500, valLoss: 30404.789062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 14] loss: 36561.521167, trainLoss: 36561.917969, valLoss: 26886.824219\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 15] loss: 36279.072047, trainLoss: 36277.488281, valLoss: 26512.556641\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 16] loss: 32973.369464, trainLoss: 32972.878906, valLoss: 23145.136719\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 17] loss: 32301.713250, trainLoss: 32301.744141, valLoss: 21481.021484\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 18] loss: 32220.142383, trainLoss: 32219.439453, valLoss: 25715.796875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 19] loss: 30521.138510, trainLoss: 30520.466797, valLoss: 20112.763672\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 20] loss: 28832.268053, trainLoss: 28832.869141, valLoss: 20307.548828\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 21] loss: 26978.985868, trainLoss: 26978.662109, valLoss: 18042.302734\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 22] loss: 27453.100916, trainLoss: 27452.884766, valLoss: 18827.800781\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 23] loss: 33695.686037, trainLoss: 33694.503906, valLoss: 20398.070312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 24] loss: 28570.736021, trainLoss: 28572.009766, valLoss: 22517.539062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 25] loss: 30190.780988, trainLoss: 30191.458984, valLoss: 21169.515625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 26] loss: 26301.871321, trainLoss: 26302.179688, valLoss: 17513.314453\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 27] loss: 23691.931945, trainLoss: 23692.113281, valLoss: 14244.273438\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 28] loss: 21574.109675, trainLoss: 21574.054688, valLoss: 12783.964844\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 29] loss: 19335.121110, trainLoss: 19335.490234, valLoss: 13679.366211\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 30] loss: 18858.129810, trainLoss: 18858.171875, valLoss: 13521.960938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 31] loss: 17710.892366, trainLoss: 17711.701172, valLoss: 10803.115234\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 32] loss: 17995.755357, trainLoss: 17995.761719, valLoss: 11306.007812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 33] loss: 17177.421809, trainLoss: 17177.664062, valLoss: 12033.056641\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 34] loss: 23065.221114, trainLoss: 23065.798828, valLoss: 13747.271484\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 35] loss: 18565.373000, trainLoss: 18565.521484, valLoss: 11685.458984\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 36] loss: 15707.679613, trainLoss: 15707.484375, valLoss: 10710.356445\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 37] loss: 15026.462141, trainLoss: 15026.913086, valLoss: 10886.432617\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 38] loss: 101100.883636, trainLoss: 101102.093750, valLoss: 13629.982422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 39] loss: 17485.604155, trainLoss: 17485.230469, valLoss: 12193.019531\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 40] loss: 16198.301147, trainLoss: 16198.729492, valLoss: 10958.958984\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 41] loss: 63105.077268, trainLoss: 63106.207031, valLoss: 12079.707031\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 42] loss: 17080.732904, trainLoss: 17081.007812, valLoss: 12074.814453\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 43] loss: 15448.304471, trainLoss: 15448.461914, valLoss: 11064.602539\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 44] loss: 14450.706916, trainLoss: 14450.865234, valLoss: 10032.360352\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 45] loss: 13808.409884, trainLoss: 13808.478516, valLoss: 9211.549805\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 46] loss: 13099.188147, trainLoss: 13099.333984, valLoss: 8967.116211\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 47] loss: 12537.374759, trainLoss: 12537.964844, valLoss: 9721.676758\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 48] loss: 12279.710587, trainLoss: 12279.549805, valLoss: 8560.841797\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 49] loss: 11979.342057, trainLoss: 11979.373047, valLoss: 7847.458008\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 50] loss: 11348.482829, trainLoss: 11348.372070, valLoss: 8261.177734\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 51] loss: 10542.767267, trainLoss: 10542.966797, valLoss: 50305.144531\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 1] loss: 2413708.862200, trainLoss: 2413807.000000, valLoss: 459088.718750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 2] loss: 1047332.953050, trainLoss: 1047515.937500, valLoss: 457942.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 3] loss: 2100695.770150, trainLoss: 2100917.500000, valLoss: 458887.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 4] loss: 2205357.229800, trainLoss: 2205380.500000, valLoss: 458949.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 5] loss: 1095744.657650, trainLoss: 1095665.000000, valLoss: 452399.593750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 6] loss: 611602.270800, trainLoss: 611601.750000, valLoss: 445194.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 7] loss: 800229.199250, trainLoss: 800228.687500, valLoss: 438607.718750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 8] loss: 2618169.263000, trainLoss: 2618175.750000, valLoss: 416504.562500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 9] loss: 572113.908950, trainLoss: 572110.937500, valLoss: 365740.343750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 10] loss: 393877.862900, trainLoss: 393879.125000, valLoss: 300873.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 11] loss: 277644.667525, trainLoss: 277644.750000, valLoss: 215685.171875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 12] loss: 212180.780675, trainLoss: 212178.203125, valLoss: 149798.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 13] loss: 147105.541525, trainLoss: 147104.359375, valLoss: 122798.054688\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 14] loss: 129184.351150, trainLoss: 129184.562500, valLoss: 117200.851562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 15] loss: 123324.476044, trainLoss: 123324.218750, valLoss: 115139.671875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 16] loss: 126689.654169, trainLoss: 126688.601562, valLoss: 114472.226562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 17] loss: 116412.633381, trainLoss: 116413.882812, valLoss: 110860.671875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 18] loss: 117221.711694, trainLoss: 117222.445312, valLoss: 110279.960938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 19] loss: 113334.054125, trainLoss: 113333.796875, valLoss: 105104.226562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 20] loss: 115115.090300, trainLoss: 115114.671875, valLoss: 104517.992188\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 21] loss: 112659.669462, trainLoss: 112659.429688, valLoss: 100764.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 22] loss: 108643.148194, trainLoss: 108642.390625, valLoss: 99508.710938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 23] loss: 103927.951231, trainLoss: 103928.312500, valLoss: 100944.632812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 24] loss: 102612.468375, trainLoss: 102611.539062, valLoss: 97460.351562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 25] loss: 100488.405419, trainLoss: 100487.609375, valLoss: 94788.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 26] loss: 96012.923769, trainLoss: 96013.351562, valLoss: 95100.570312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 27] loss: 94914.790306, trainLoss: 94914.804688, valLoss: 91412.179688\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 28] loss: 100908.297100, trainLoss: 100909.062500, valLoss: 93760.226562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 29] loss: 151724.551062, trainLoss: 151725.812500, valLoss: 93739.085938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 30] loss: 95687.481762, trainLoss: 95687.906250, valLoss: 96267.492188\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 31] loss: 93375.483619, trainLoss: 93375.757812, valLoss: 86376.085938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 32] loss: 86669.923825, trainLoss: 86669.109375, valLoss: 82811.929688\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 33] loss: 91452.710744, trainLoss: 91452.343750, valLoss: 85031.343750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 34] loss: 87824.214013, trainLoss: 87824.015625, valLoss: 86222.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 35] loss: 84701.990844, trainLoss: 84702.343750, valLoss: 79996.226562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 36] loss: 82415.301031, trainLoss: 82415.507812, valLoss: 77697.664062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 37] loss: 82891.587269, trainLoss: 82892.687500, valLoss: 75429.710938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 38] loss: 73575.679259, trainLoss: 73573.390625, valLoss: 73191.140625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 39] loss: 76460.974484, trainLoss: 76462.484375, valLoss: 72791.281250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 40] loss: 72770.805181, trainLoss: 72771.156250, valLoss: 72175.867188\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 41] loss: 72336.448678, trainLoss: 72336.632812, valLoss: 72283.132812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 42] loss: 67800.913838, trainLoss: 67800.601562, valLoss: 65129.136719\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 43] loss: 66601.238766, trainLoss: 66601.117188, valLoss: 65301.113281\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 44] loss: 62936.372547, trainLoss: 62936.164062, valLoss: 60653.457031\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 45] loss: 64266.991581, trainLoss: 64267.488281, valLoss: 63203.203125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 46] loss: 67433.878056, trainLoss: 67434.750000, valLoss: 59932.855469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 47] loss: 60996.739950, trainLoss: 60996.644531, valLoss: 55410.207031\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 48] loss: 59189.527914, trainLoss: 59190.066406, valLoss: 55320.972656\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 49] loss: 57388.702631, trainLoss: 57389.488281, valLoss: 56928.191406\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 50] loss: 56313.552969, trainLoss: 56312.304688, valLoss: 50367.550781\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 51] loss: 52962.182928, trainLoss: 52961.785156, valLoss: 48008.285156\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 52] loss: 52410.052738, trainLoss: 52410.125000, valLoss: 48167.914062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 53] loss: 49766.759113, trainLoss: 49767.773438, valLoss: 43974.292969\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 54] loss: 49153.532297, trainLoss: 49153.988281, valLoss: 42047.324219\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 55] loss: 51520.962678, trainLoss: 51521.437500, valLoss: 43207.472656\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 56] loss: 66827.922300, trainLoss: 66828.000000, valLoss: 54061.906250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 57] loss: 53603.506600, trainLoss: 53602.515625, valLoss: 47094.007812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 58] loss: 58599.367925, trainLoss: 58599.308594, valLoss: 55372.093750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 59] loss: 52307.959308, trainLoss: 52308.183594, valLoss: 46827.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 60] loss: 51105.901509, trainLoss: 51106.285156, valLoss: 51108.683594\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 61] loss: 48710.103062, trainLoss: 48709.402344, valLoss: 45997.902344\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 62] loss: 43914.438944, trainLoss: 43914.589844, valLoss: 46193.414062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 63] loss: 43175.239000, trainLoss: 43175.597656, valLoss: 47381.808594\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 64] loss: 44868.265808, trainLoss: 44867.164062, valLoss: 59326.390625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 65] loss: 41607.691422, trainLoss: 41608.734375, valLoss: 40827.015625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 66] loss: 47936.777498, trainLoss: 47936.820312, valLoss: 44836.257812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 67] loss: 48214.633533, trainLoss: 48214.765625, valLoss: 39452.015625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 68] loss: 40464.919555, trainLoss: 40464.773438, valLoss: 41512.601562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 69] loss: 40915.771769, trainLoss: 40915.949219, valLoss: 37355.074219\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 70] loss: 38612.229458, trainLoss: 38612.144531, valLoss: 39355.804688\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 71] loss: 35210.510062, trainLoss: 35210.964844, valLoss: 39316.816406\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 72] loss: 72818.935539, trainLoss: 72818.601562, valLoss: 50288.410156\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convergence params\n",
    "movingWindowSize = 20\n",
    "\n",
    "# multiprocessing cpu params\n",
    "numWorkers = 4\n",
    "\n",
    "# define dataset variables to adapt to dataloader format\n",
    "\n",
    "# training set. divide into batches inside the training loop\n",
    "trainingListIDs = df_train.index.get_level_values('trace_name').tolist()\n",
    "trainingLabels = (df_train['p_arrival_sample']).to_dict()\n",
    "\n",
    "\n",
    "# validation set\n",
    "validationListIDs = df_val.index.get_level_values('trace_name').tolist()\n",
    "validationLabels = (df_val['p_arrival_sample']).to_dict()\n",
    "\n",
    "validationSet = Dataset(validationListIDs, validationLabels)\n",
    "validationGenerator = torch.utils.data.DataLoader(validationSet, batch_size = 256, num_workers = numWorkers)\n",
    "\n",
    "# test set\n",
    "testListIDs = df_test.index.get_level_values('trace_name').tolist()\n",
    "testLabels = (df_test['p_arrival_sample']).to_dict()\n",
    "\n",
    "testSet = Dataset(testListIDs, testLabels)\n",
    "testGenerator = torch.utils.data.DataLoader(testSet, batch_size = 256, num_workers = numWorkers)\n",
    "    \n",
    "# hyperParameters\n",
    "numEpochs = 200\n",
    "batchSize = 64\n",
    "learningRateList = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "dropOutProbList = [0.3, 0.5, 0.7]\n",
    "#learningRateList = [1e-3]\n",
    "#dropOutProbList = [0.5]\n",
    "\n",
    "# training\n",
    "lossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "lossProgression[:] = np.nan\n",
    "\n",
    "trainLossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "trainLossProgression[:] = np.nan\n",
    "\n",
    "valLossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "valLossProgression[:] = np.nan\n",
    "\n",
    "bestValLoss = np.zeros((len(dropOutProbList), len(learningRateList)))\n",
    "bestValLoss[:] = np.inf\n",
    "\n",
    "\n",
    "for d,dropOutProb in enumerate (dropOutProbList):\n",
    "    \n",
    "    numBatches = int(len(df_train)/batchSize) + 1 if len(df_train) % batchSize != 0 else int(len(df_train)/batchSize)\n",
    "    \n",
    "    # invoke dataloader with appropriate batchSize\n",
    "    trainingSet = Dataset(trainingListIDs, trainingLabels)\n",
    "    trainingGenerator = torch.utils.data.DataLoader(trainingSet, batch_size = batchSize, shuffle=True, num_workers = numWorkers)        \n",
    "    \n",
    "    for l,learningRate in enumerate(learningRateList):\n",
    "        \n",
    "        # create a neuralNet object\n",
    "        net = Net(dropOutProb)        \n",
    "        net.to(device)\n",
    "        \n",
    "        \n",
    "        # optimizer type\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learningRate)\n",
    "        \n",
    "        # movingWindow of previous epochs to check for convergence\n",
    "        deltaValLossHistory = []\n",
    "        prevValLoss = np.inf        \n",
    "        \n",
    "        fileTag = f'batchSize_{batchSize}_learningRate_{learningRate}_dropOutProb_{dropOutProb}_numEpochs_{numEpochs}_dataFraction_{earthquakeSampleFraction}_{noiseSampleFraction}'\n",
    "        \n",
    "        for epoch in range(numEpochs):                        \n",
    "            \n",
    "            # set network in training mode\n",
    "            net.train()\n",
    "            \n",
    "            runningLoss = 0.0\n",
    "            sqError = 0.0\n",
    "            for i, data in enumerate(trainingGenerator, 0):\n",
    "\n",
    "                inputBatch, groundTruthBatch = data\n",
    "                groundTruthBatch = torch.tensor(groundTruthBatch, dtype = torch.float32)\n",
    "                inputBatch, groundTruthBatch = inputBatch.to(device), groundTruthBatch.to(device)\n",
    "                                                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputPredBatch = net(inputBatch)\n",
    "                \n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(outputPredBatch,groundTruthBatch.unsqueeze(1))\n",
    "                \n",
    "                # backprop\n",
    "                loss.backward()\n",
    "                \n",
    "                # gradient descent step\n",
    "                optimizer.step()    \n",
    "                \n",
    "                # output prediction label\n",
    "                outputPredLabelBatch = torch.round(outputPredBatch)\n",
    "\n",
    "                # collect training loss, accuracy statistics\n",
    "                if i == numBatches - 1:\n",
    "                    runningLoss += loss.item() * (len(df_train) - i*batchSize)                    \n",
    "                else:                        \n",
    "                    runningLoss += loss.item() * batchSize\n",
    "                \n",
    "                sqError += torch.sum((outputPredLabelBatch.squeeze() - groundTruthBatch)**2)\n",
    "            \n",
    "            # track training loss/accuracy Progression\n",
    "            avgLoss = runningLoss/len(df_train)\n",
    "            lossProgression[d, l, epoch] = avgLoss\n",
    "            trainLoss = sqError/len(df_train)\n",
    "            trainLossProgression[d,l, epoch] = trainLoss\n",
    "                        \n",
    "            # evaluate validation set accuracy\n",
    "            \n",
    "            # set network in eval mode\n",
    "            net.eval()\n",
    "            \n",
    "            sqError = 0.0\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for i, data in enumerate(validationGenerator, 0):\n",
    "\n",
    "                    inputBatch, groundTruthBatch = data\n",
    "                    groundTruthBatch = torch.tensor(groundTruthBatch, dtype = torch.float32)\n",
    "                    inputBatch, groundTruthBatch = inputBatch.to(device), groundTruthBatch.to(device)\n",
    "                    outputPredBatch = net(inputBatch)\n",
    "                    \n",
    "                    # output prediction label\n",
    "                    outputPredLabelBatch = torch.round(outputPredBatch)\n",
    "                    \n",
    "                    sqError += torch.sum((outputPredLabelBatch.squeeze() - groundTruthBatch)**2)\n",
    "\n",
    "            valLoss = sqError/len(df_val)\n",
    "            valLossProgression[d,l,epoch] = valLoss\n",
    "                        \n",
    "            print('[batchSize = %d, dropOutProb = %.2g, learningRate = %.2g, epoch = %d] loss: %f, trainLoss: %f, valLoss: %f' %\n",
    "                                                (batchSize, dropOutProb, learningRate, epoch + 1, avgLoss, trainLoss, valLoss))\n",
    "            \n",
    "            # update network parameters if validation accuracy in current epoch is better than the past            \n",
    "            if valLoss < bestValLoss[d,l]:\n",
    "                \n",
    "                # store the neuralNet parameters in a file                \n",
    "                path = f'{netParamsDirName}/cnn3Layer_12_12_8_fc2layer_16_16_{fileTag}.pth'\n",
    "                torch.save(net.state_dict(), path)\n",
    "                \n",
    "                bestValLoss[d,l] = valLoss\n",
    "            \n",
    "            # check for convergence and exit early if valLoss is not improving\n",
    "            deltaValLoss = valLoss - prevValLoss\n",
    "            deltaValLossHistory.insert(0, deltaValLoss)            \n",
    "            \n",
    "            if epoch >= movingWindowSize:\n",
    "                deltaValLossHistory.pop()\n",
    "                \n",
    "                if sum(deltaValLossHistory) >= 0:\n",
    "                    print ('Validation loss starting to increase. Exiting...')\n",
    "                    break\n",
    "                \n",
    "            prevValLoss = valLoss                    \n",
    "        \n",
    "        print ('Finished training...')\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.48844931320997\n"
     ]
    }
   ],
   "source": [
    "print (np.min(bestValLoss)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
