{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# find GPU device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'dataFiles'\n",
    "datasetSize = '100K'\n",
    "earthquakeSampleFraction = 0.25\n",
    "noiseSampleFraction = 0.25\n",
    "csvFileName = f'{dataDir}/filtered_earthquakeSampleFraction_{earthquakeSampleFraction}_noiseSampleFraction_{noiseSampleFraction}.csv'\n",
    "hdf5FileName = f'{dataDir}/filtered_earthquakeSampleFraction_{earthquakeSampleFraction}_noiseSampleFraction_{noiseSampleFraction}.hdf5'\n",
    "\n",
    "trainSetFraction = 0.8\n",
    "valSetFraction = 0.1\n",
    "testSetFraction = 1 - (trainSetFraction + valSetFraction) \n",
    "\n",
    "# read the csv file\n",
    "df_csv = pd.read_csv(csvFileName)\n",
    "\n",
    "# set trace name as index\n",
    "df_csv.set_index(['trace_name'], inplace=True)\n",
    "\n",
    "# keep rows with valid data\n",
    "df_csv.drop(df_csv[df_csv['s_arrival_sample'].isna()].index, inplace=True)\n",
    "\n",
    "# split the dataset into train, validation and test\n",
    "df_train, df_val, df_test = np.split(df_csv.sample(frac = 1), [int(trainSetFraction*len(df_csv)), int((trainSetFraction + valSetFraction)*len(df_csv))])\n",
    "\n",
    "# read the hdf5 file\n",
    "hdf5Data = h5py.File(hdf5FileName, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length = 40000\n",
      "Val set length = 5000\n",
      "Test set length = 5000\n"
     ]
    }
   ],
   "source": [
    "print ('Train set length =', len(df_train))\n",
    "print ('Val set length =', len(df_val))\n",
    "print ('Test set length =', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define other relevant dirNames\n",
    "\n",
    "# dir to dump plots\n",
    "plotImgDir = 'plotImages/cnn1dSArrivalRegression'\n",
    "os.system(f'mkdir -p {plotImgDir}')\n",
    "\n",
    "# directories to dump neuralNet params\n",
    "netParamsDirName = 'netParams/cnn1dSArrivalRegression'\n",
    "os.system(f'mkdir -p {netParamsDirName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, listIDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.listIDs = listIDs\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.listIDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.listIDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = hdf5Data.get('data/' + ID)\n",
    "        X = np.array(X)\n",
    "        X = X.T\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Net(\n",
      "  (conv1): Conv1d(3, 12, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(12, 12, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(12, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (poolDiv2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (poolDiv4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropOut): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=3000, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# define a 1D convolutional neural network\n",
    "\n",
    "dropOutProb = 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropOutProb = 0.5):\n",
    "        \n",
    "        # inherit base class\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels = 3, out_channels = 12, kernel_size=5, stride = 1, padding = 2)\n",
    "                \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels = 12, out_channels = 12, kernel_size=5, stride = 1, padding = 2)\n",
    "                \n",
    "        # layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels = 12, out_channels = 8, kernel_size=5, stride = 1, padding = 2)\n",
    "        \n",
    "        # use downsampling by 2 for 1st 2 layers, and then downsample by 4 for the 3rd\n",
    "        self.poolDiv2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.poolDiv4 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropOut = nn.Dropout(p = dropOutProb)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features = 375 * 8, out_features = 16)\n",
    "        self.fc2 = nn.Linear(in_features = 16, out_features = 16)\n",
    "        self.fc3 = nn.Linear(in_features = 16, out_features = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.poolDiv2(F.relu(self.conv1(x)))\n",
    "        x = self.poolDiv2(F.relu(self.conv2(x)))\n",
    "        x = self.poolDiv4(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.dropOut(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):        \n",
    "        size = x.size()[1:]\n",
    "        num_features = 1        \n",
    "        for s in size:\n",
    "            num_features *= s            \n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net(dropOutProb)\n",
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss criteria\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 1] loss: 1412191410.648600, trainLoss: 1412192000.000000, valLoss: 1675687.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 2] loss: 1643607.539400, trainLoss: 1643606.750000, valLoss: 1647086.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 3] loss: 1605980.480800, trainLoss: 1605990.750000, valLoss: 1602389.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 4] loss: 1553772.883000, trainLoss: 1553774.500000, valLoss: 1542396.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 5] loss: 1487008.408300, trainLoss: 1486994.375000, valLoss: 1468197.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 6] loss: 1406988.613200, trainLoss: 1406996.000000, valLoss: 1381179.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 7] loss: 1316709.478800, trainLoss: 1316708.000000, valLoss: 1287109.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 8] loss: 1220221.205700, trainLoss: 1220219.125000, valLoss: 1189283.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 9] loss: 1121602.167800, trainLoss: 1121610.125000, valLoss: 1090720.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 10] loss: 1024205.535300, trainLoss: 1024210.000000, valLoss: 995826.437500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 11] loss: 930379.282200, trainLoss: 930375.437500, valLoss: 903291.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 12] loss: 841515.303800, trainLoss: 841523.937500, valLoss: 816807.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 13] loss: 758426.007700, trainLoss: 758428.375000, valLoss: 737781.062500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 14] loss: 681556.882100, trainLoss: 681557.312500, valLoss: 663286.437500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 15] loss: 611057.108850, trainLoss: 611051.312500, valLoss: 596032.312500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 16] loss: 547037.160350, trainLoss: 547040.312500, valLoss: 534610.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 17] loss: 489523.750700, trainLoss: 489520.437500, valLoss: 480974.937500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 18] loss: 438370.243025, trainLoss: 438370.437500, valLoss: 432747.656250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 19] loss: 393401.289925, trainLoss: 393399.312500, valLoss: 390700.843750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 20] loss: 354595.131575, trainLoss: 354597.843750, valLoss: 354528.437500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 21] loss: 321621.616712, trainLoss: 321624.937500, valLoss: 324507.031250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 22] loss: 294308.973762, trainLoss: 294305.437500, valLoss: 300060.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 23] loss: 272344.218950, trainLoss: 272346.468750, valLoss: 280636.781250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 24] loss: 255341.688888, trainLoss: 255340.031250, valLoss: 265707.937500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 25] loss: 242847.172187, trainLoss: 242847.375000, valLoss: 255240.953125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 26] loss: 234264.929100, trainLoss: 234266.125000, valLoss: 248323.781250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 27] loss: 228937.883262, trainLoss: 228938.765625, valLoss: 244363.265625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 28] loss: 226054.074538, trainLoss: 226054.593750, valLoss: 242351.406250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 29] loss: 224712.510038, trainLoss: 224710.218750, valLoss: 241534.656250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 30] loss: 224215.990737, trainLoss: 224217.078125, valLoss: 241260.687500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 31] loss: 224068.651550, trainLoss: 224067.578125, valLoss: 241178.406250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 32] loss: 224030.589275, trainLoss: 224030.937500, valLoss: 241153.093750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 33] loss: 224025.532625, trainLoss: 224028.328125, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 34] loss: 224023.392800, trainLoss: 224023.437500, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 35] loss: 224020.537500, trainLoss: 224018.843750, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 36] loss: 224022.282350, trainLoss: 224024.031250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 37] loss: 224022.311363, trainLoss: 224023.859375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 38] loss: 224020.267087, trainLoss: 224020.500000, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 39] loss: 224021.852400, trainLoss: 224021.578125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 40] loss: 224021.374750, trainLoss: 224021.015625, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 41] loss: 224020.606637, trainLoss: 224019.859375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 42] loss: 224023.533587, trainLoss: 224022.546875, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 43] loss: 224020.066187, trainLoss: 224020.812500, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 44] loss: 224023.128925, trainLoss: 224021.500000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 45] loss: 224022.613963, trainLoss: 224024.281250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 46] loss: 224021.860050, trainLoss: 224019.171875, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 47] loss: 224020.001512, trainLoss: 224020.953125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 48] loss: 224020.714838, trainLoss: 224022.515625, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 49] loss: 224020.190725, trainLoss: 224018.140625, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 50] loss: 224021.775063, trainLoss: 224025.281250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 51] loss: 224021.126850, trainLoss: 224021.953125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 52] loss: 224021.351600, trainLoss: 224021.453125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 53] loss: 224022.583863, trainLoss: 224020.468750, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.1, epoch = 54] loss: 224022.167700, trainLoss: 224020.390625, valLoss: 241146.187500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 1] loss: 2949585.330425, trainLoss: 2949569.250000, valLoss: 585873.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 2] loss: 525619.075937, trainLoss: 525620.062500, valLoss: 514521.281250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 3] loss: 465382.801975, trainLoss: 465380.437500, valLoss: 444219.781250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 4] loss: 2261876.210913, trainLoss: 2261875.500000, valLoss: 542636.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 5] loss: 479845.271569, trainLoss: 479841.062500, valLoss: 488152.468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 6] loss: 442115.844212, trainLoss: 442115.312500, valLoss: 455133.531250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 7] loss: 417010.234188, trainLoss: 417012.656250, valLoss: 527964.687500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 8] loss: 478843.336731, trainLoss: 478843.750000, valLoss: 478570.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 9] loss: 445425.514325, trainLoss: 445423.000000, valLoss: 452419.218750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 10] loss: 14461347822.849125, trainLoss: 14461301760.000000, valLoss: 530026.687500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 11] loss: 192995876.638050, trainLoss: 192995920.000000, valLoss: 549080.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 12] loss: 497056.973450, trainLoss: 497065.250000, valLoss: 515197.843750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 13] loss: 477261.755300, trainLoss: 477260.218750, valLoss: 507429.156250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 14] loss: 778651.106437, trainLoss: 778657.750000, valLoss: 459273.312500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 15] loss: 313717.303413, trainLoss: 313720.093750, valLoss: 210128.109375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 16] loss: 6815543.442563, trainLoss: 6815550.500000, valLoss: 184012.187500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 17] loss: 166061.983656, trainLoss: 166062.296875, valLoss: 158699.687500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 18] loss: 164688.073069, trainLoss: 164686.593750, valLoss: 154748.062500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 19] loss: 155813.113000, trainLoss: 155814.937500, valLoss: 152007.312500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 20] loss: 149995.931856, trainLoss: 149994.562500, valLoss: 149421.046875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 21] loss: 148200.731394, trainLoss: 148200.578125, valLoss: 144720.968750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 22] loss: 144413.808137, trainLoss: 144416.312500, valLoss: 146967.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 23] loss: 143714.492619, trainLoss: 143715.562500, valLoss: 146892.421875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 24] loss: 141300.835031, trainLoss: 141300.281250, valLoss: 140356.515625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 25] loss: 144150.423650, trainLoss: 144152.375000, valLoss: 141518.593750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 26] loss: 138268.305600, trainLoss: 138267.093750, valLoss: 138158.421875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.01, epoch = 27] loss: 57000154.670606, trainLoss: 57000136.000000, valLoss: 1271189.125000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 1] loss: 2232565.725600, trainLoss: 2232569.500000, valLoss: 1583474.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 2] loss: 1083791.828350, trainLoss: 1083788.250000, valLoss: 671616.937500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 3] loss: 625500.184463, trainLoss: 625500.750000, valLoss: 2607069.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 4] loss: 810467.241900, trainLoss: 810473.812500, valLoss: 512102.906250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 5] loss: 499996.667338, trainLoss: 499989.187500, valLoss: 464561.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 6] loss: 405080.515187, trainLoss: 405086.000000, valLoss: 411837.031250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 7] loss: 381334.374494, trainLoss: 381333.593750, valLoss: 400294.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 8] loss: 636511.884119, trainLoss: 636510.375000, valLoss: 366779.406250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 9] loss: 358325.629681, trainLoss: 358323.937500, valLoss: 364444.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 10] loss: 315761.178144, trainLoss: 315765.406250, valLoss: 313093.156250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 11] loss: 340329.461031, trainLoss: 340327.406250, valLoss: 336657.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 12] loss: 291031.697144, trainLoss: 291035.875000, valLoss: 280649.093750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 13] loss: 281208.492231, trainLoss: 281205.125000, valLoss: 272924.812500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 14] loss: 241644.176662, trainLoss: 241646.046875, valLoss: 215377.203125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 15] loss: 256746.776131, trainLoss: 256743.078125, valLoss: 287638.687500\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 16] loss: 258108.855394, trainLoss: 258108.484375, valLoss: 268438.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 17] loss: 220629.506619, trainLoss: 220629.578125, valLoss: 181683.703125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 18] loss: 288063.331825, trainLoss: 288062.062500, valLoss: 239908.343750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 19] loss: 271251.541288, trainLoss: 271253.625000, valLoss: 247132.609375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 20] loss: 220068.165631, trainLoss: 220066.578125, valLoss: 203386.859375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 21] loss: 176406.549819, trainLoss: 176406.687500, valLoss: 177501.140625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 22] loss: 142029.264312, trainLoss: 142029.046875, valLoss: 141162.484375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 23] loss: 147521.214309, trainLoss: 147519.171875, valLoss: 146281.781250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 24] loss: 128772.956381, trainLoss: 128774.164062, valLoss: 115704.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 25] loss: 201601.708122, trainLoss: 201601.640625, valLoss: 226894.656250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 26] loss: 187098.505853, trainLoss: 187098.625000, valLoss: 167422.046875\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 27] loss: 158949.712178, trainLoss: 158950.203125, valLoss: 187620.203125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 28] loss: 126919.681503, trainLoss: 126918.703125, valLoss: 237803.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 29] loss: 132163.387556, trainLoss: 132163.640625, valLoss: 134060.078125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 30] loss: 120272.659672, trainLoss: 120273.406250, valLoss: 129384.984375\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 31] loss: 101384.236950, trainLoss: 101383.734375, valLoss: 103349.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 32] loss: 102633.226092, trainLoss: 102632.578125, valLoss: 93182.210938\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 33] loss: 94260.759634, trainLoss: 94260.664062, valLoss: 94140.218750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 34] loss: 90184.591000, trainLoss: 90184.460938, valLoss: 91144.515625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 35] loss: 125157.310900, trainLoss: 125156.156250, valLoss: 131950.031250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 36] loss: 116814.227958, trainLoss: 116813.875000, valLoss: 110986.507812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 37] loss: 101940.888413, trainLoss: 101939.164062, valLoss: 97332.289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 38] loss: 85251.967148, trainLoss: 85252.890625, valLoss: 85698.281250\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 39] loss: 87482.604106, trainLoss: 87483.734375, valLoss: 81128.179688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 40] loss: 101936.690378, trainLoss: 101937.804688, valLoss: 125829.593750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 41] loss: 89880.864937, trainLoss: 89881.226562, valLoss: 76914.257812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 42] loss: 74636.673095, trainLoss: 74637.195312, valLoss: 75124.929688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 43] loss: 90748.692072, trainLoss: 90748.640625, valLoss: 102550.507812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 44] loss: 86595.114973, trainLoss: 86595.226562, valLoss: 82085.515625\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 45] loss: 73375.217086, trainLoss: 73374.000000, valLoss: 66444.468750\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 46] loss: 86673.631083, trainLoss: 86674.289062, valLoss: 68002.804688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 47] loss: 74487.295281, trainLoss: 74486.593750, valLoss: 78816.132812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 48] loss: 83998.007545, trainLoss: 83996.609375, valLoss: 78473.382812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 49] loss: 96173.961456, trainLoss: 96173.281250, valLoss: 78829.328125\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 50] loss: 70390.231728, trainLoss: 70389.859375, valLoss: 66175.679688\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 51] loss: 80833.872530, trainLoss: 80834.484375, valLoss: 85096.007812\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.001, epoch = 52] loss: 87482.351083, trainLoss: 87483.585938, valLoss: 108053.703125\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 1] loss: 1663489.858400, trainLoss: 1663473.375000, valLoss: 1665221.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 2] loss: 1653437.930800, trainLoss: 1653351.500000, valLoss: 1658224.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 3] loss: 1627135.697200, trainLoss: 1627169.500000, valLoss: 1656759.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 4] loss: 1607366.561200, trainLoss: 1607378.750000, valLoss: 1632201.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 5] loss: 1589348.396400, trainLoss: 1589346.000000, valLoss: 1808859.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 6] loss: 1580984.466800, trainLoss: 1580984.000000, valLoss: 1618334.375000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 7] loss: 1548219.478300, trainLoss: 1548221.625000, valLoss: 1820518.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 8] loss: 1522101.350200, trainLoss: 1522099.875000, valLoss: 1837735.125000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 9] loss: 1457440.374700, trainLoss: 1457441.375000, valLoss: 1788821.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 10] loss: 1314145.228400, trainLoss: 1314147.250000, valLoss: 1749742.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 11] loss: 1016158.335550, trainLoss: 1016153.500000, valLoss: 1335460.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 12] loss: 631830.388975, trainLoss: 631834.375000, valLoss: 2680925.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 13] loss: 508270.893175, trainLoss: 508268.750000, valLoss: 1641531.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 14] loss: 501300.453025, trainLoss: 501299.125000, valLoss: 650922.250000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 15] loss: 524498.529250, trainLoss: 524501.250000, valLoss: 2155540.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 16] loss: 458497.529775, trainLoss: 458495.781250, valLoss: 1869333.500000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 17] loss: 428611.978900, trainLoss: 428614.062500, valLoss: 1945640.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 18] loss: 414700.211413, trainLoss: 414700.843750, valLoss: 2380426.000000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 19] loss: 390020.568288, trainLoss: 390021.468750, valLoss: 2752042.750000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 20] loss: 380166.614450, trainLoss: 380169.375000, valLoss: 1627339.625000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 21] loss: 377281.282500, trainLoss: 377281.000000, valLoss: 628908.875000\n",
      "[batchSize = 64, dropOutProb = 0.3, learningRate = 0.0001, epoch = 22] loss: 370108.881037, trainLoss: 370110.031250, valLoss: 2515998.250000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 1] loss: 152064788846.735413, trainLoss: 152063131648.000000, valLoss: 1690096.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 2] loss: 1669562.966000, trainLoss: 1669679.250000, valLoss: 1687689.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 3] loss: 1665427.504400, trainLoss: 1665464.750000, valLoss: 1682882.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 4] loss: 1659285.855200, trainLoss: 1659283.625000, valLoss: 1673293.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 5] loss: 1650595.113600, trainLoss: 1650531.125000, valLoss: 1663735.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 6] loss: 1638580.929000, trainLoss: 1638598.000000, valLoss: 1649458.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 7] loss: 1622196.529200, trainLoss: 1622189.250000, valLoss: 1630535.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 8] loss: 1600108.687200, trainLoss: 1600115.125000, valLoss: 1604724.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 9] loss: 1570687.095600, trainLoss: 1570672.875000, valLoss: 1572224.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 10] loss: 1532112.207800, trainLoss: 1532116.125000, valLoss: 1528743.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 11] loss: 1482662.879000, trainLoss: 1482664.375000, valLoss: 1472632.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 12] loss: 1421179.564500, trainLoss: 1421190.250000, valLoss: 1404790.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 13] loss: 1347752.417400, trainLoss: 1347751.250000, valLoss: 1326333.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 14] loss: 1264155.416500, trainLoss: 1264170.625000, valLoss: 1238594.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 15] loss: 1173650.870900, trainLoss: 1173657.750000, valLoss: 1145021.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 16] loss: 1080012.027200, trainLoss: 1080023.500000, valLoss: 1050648.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 17] loss: 986713.173400, trainLoss: 986706.687500, valLoss: 959781.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 18] loss: 896313.461950, trainLoss: 896318.437500, valLoss: 871142.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 19] loss: 810396.109500, trainLoss: 810399.062500, valLoss: 786858.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 20] loss: 730047.416550, trainLoss: 730048.562500, valLoss: 709992.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 21] loss: 655722.687200, trainLoss: 655724.250000, valLoss: 638957.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 22] loss: 587702.004900, trainLoss: 587704.375000, valLoss: 573755.812500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 23] loss: 526086.543150, trainLoss: 526084.562500, valLoss: 515432.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 24] loss: 470828.792625, trainLoss: 470829.593750, valLoss: 462726.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 25] loss: 421849.490300, trainLoss: 421843.968750, valLoss: 417313.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 26] loss: 379041.149400, trainLoss: 379049.156250, valLoss: 377102.812500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 27] loss: 342282.060500, trainLoss: 342285.031250, valLoss: 343368.812500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 28] loss: 311368.443787, trainLoss: 311366.531250, valLoss: 315523.843750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 29] loss: 285954.776238, trainLoss: 285954.968750, valLoss: 292549.312500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 30] loss: 265785.244138, trainLoss: 265785.218750, valLoss: 274900.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 31] loss: 250416.210000, trainLoss: 250415.437500, valLoss: 261515.718750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 32] loss: 239382.941700, trainLoss: 239382.671875, valLoss: 252535.593750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 33] loss: 232065.412188, trainLoss: 232067.703125, valLoss: 246729.312500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 34] loss: 227679.895512, trainLoss: 227680.140625, valLoss: 243423.218750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 35] loss: 225402.274100, trainLoss: 225402.468750, valLoss: 241914.281250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 36] loss: 224450.423875, trainLoss: 224448.328125, valLoss: 241362.484375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 37] loss: 224130.389563, trainLoss: 224128.968750, valLoss: 241205.312500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 38] loss: 224043.179513, trainLoss: 224047.406250, valLoss: 241153.093750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 39] loss: 224023.545475, trainLoss: 224023.875000, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 40] loss: 224020.293800, trainLoss: 224021.093750, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 41] loss: 224024.720312, trainLoss: 224025.421875, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 42] loss: 224021.305650, trainLoss: 224021.812500, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 43] loss: 224022.821488, trainLoss: 224025.109375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 44] loss: 224020.923775, trainLoss: 224020.328125, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 45] loss: 224019.906175, trainLoss: 224016.921875, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 46] loss: 224022.628737, trainLoss: 224023.234375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 47] loss: 224020.953838, trainLoss: 224020.734375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 48] loss: 224021.605712, trainLoss: 224026.078125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 49] loss: 224019.882313, trainLoss: 224021.468750, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 50] loss: 224020.821675, trainLoss: 224021.828125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 51] loss: 224021.873650, trainLoss: 224023.140625, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 52] loss: 224021.413587, trainLoss: 224023.390625, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 53] loss: 224022.574775, trainLoss: 224021.734375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 54] loss: 224023.305525, trainLoss: 224020.859375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 55] loss: 224023.330550, trainLoss: 224024.625000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 56] loss: 224022.786925, trainLoss: 224022.234375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 57] loss: 224021.233712, trainLoss: 224020.656250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 58] loss: 224021.443487, trainLoss: 224020.500000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 59] loss: 224021.895500, trainLoss: 224020.093750, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 60] loss: 224024.316750, trainLoss: 224025.062500, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.1, epoch = 61] loss: 224021.609288, trainLoss: 224022.218750, valLoss: 241146.187500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 1] loss: 2695702.513000, trainLoss: 2695716.000000, valLoss: 531669.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 2] loss: 459829.784987, trainLoss: 459823.500000, valLoss: 459078.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 3] loss: 398304.995538, trainLoss: 398306.843750, valLoss: 381964.781250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 4] loss: 349795.117000, trainLoss: 349793.500000, valLoss: 356556.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 5] loss: 27094350.977437, trainLoss: 27094340.000000, valLoss: 770912.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 6] loss: 638108.820350, trainLoss: 638114.500000, valLoss: 530554.812500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 7] loss: 476891.089162, trainLoss: 476885.750000, valLoss: 305566272.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 8] loss: 7072707.516550, trainLoss: 7072711.500000, valLoss: 1654114.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 9] loss: 1618045.362800, trainLoss: 1618050.625000, valLoss: 1619441.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 10] loss: 1587764.980200, trainLoss: 1587749.500000, valLoss: 1592190.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 11] loss: 222735010.095325, trainLoss: 222734752.000000, valLoss: 527608.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 12] loss: 515278.883712, trainLoss: 515275.250000, valLoss: 493810.406250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 13] loss: 466143.871237, trainLoss: 466142.093750, valLoss: 458135.093750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 14] loss: 14188697.810137, trainLoss: 14188703.000000, valLoss: 1624942.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 15] loss: 1595072.823400, trainLoss: 1595089.125000, valLoss: 1598235.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 16] loss: 1569977.935200, trainLoss: 1569958.250000, valLoss: 1575096.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 17] loss: 1546384.866800, trainLoss: 1546411.125000, valLoss: 1552464.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 18] loss: 1522900.556200, trainLoss: 1522871.000000, valLoss: 1527400.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 19] loss: 1498389.392700, trainLoss: 1498379.625000, valLoss: 1503340.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 20] loss: 1473680.769700, trainLoss: 1473678.875000, valLoss: 1480139.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.01, epoch = 21] loss: 1451181.380600, trainLoss: 1451190.000000, valLoss: 1460232.500000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 1] loss: 1712687.007200, trainLoss: 1712709.750000, valLoss: 864703.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 2] loss: 664361.931200, trainLoss: 664359.250000, valLoss: 600509.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 3] loss: 539460.708000, trainLoss: 539460.312500, valLoss: 553330.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 4] loss: 483330.832169, trainLoss: 483334.500000, valLoss: 679229.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 5] loss: 488528.602063, trainLoss: 488525.156250, valLoss: 477572.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 6] loss: 428024.798713, trainLoss: 428023.812500, valLoss: 448789.531250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 7] loss: 399834.968119, trainLoss: 399841.687500, valLoss: 406910.468750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 8] loss: 406458.612194, trainLoss: 406461.000000, valLoss: 399293.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 9] loss: 367206.706562, trainLoss: 367200.250000, valLoss: 342610.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 10] loss: 377279.844669, trainLoss: 377278.718750, valLoss: 366775.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 11] loss: 345984.153900, trainLoss: 345979.906250, valLoss: 356171.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 12] loss: 314281.863562, trainLoss: 314279.750000, valLoss: 313629.781250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 13] loss: 404541.112375, trainLoss: 404542.750000, valLoss: 326495.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 14] loss: 311937.702850, trainLoss: 311939.250000, valLoss: 348473.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 15] loss: 314477.502775, trainLoss: 314479.468750, valLoss: 323486.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 16] loss: 289420.184119, trainLoss: 289419.437500, valLoss: 285699.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 17] loss: 251006.291319, trainLoss: 251008.609375, valLoss: 216926.531250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 18] loss: 319546.760937, trainLoss: 319547.343750, valLoss: 299653.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 19] loss: 268035.980156, trainLoss: 268033.812500, valLoss: 263078.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 20] loss: 250387.268319, trainLoss: 250387.265625, valLoss: 221572.906250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 21] loss: 232742.162219, trainLoss: 232741.453125, valLoss: 249543.062500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 22] loss: 243569.771294, trainLoss: 243570.125000, valLoss: 244875.468750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 23] loss: 334107.399675, trainLoss: 334111.468750, valLoss: 238702.921875\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 24] loss: 214381.819181, trainLoss: 214380.437500, valLoss: 209956.156250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 25] loss: 189560.631203, trainLoss: 189562.312500, valLoss: 163262.453125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 26] loss: 174620.668147, trainLoss: 174619.406250, valLoss: 151930.140625\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 27] loss: 176995.427297, trainLoss: 176997.625000, valLoss: 190631.234375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 28] loss: 183093.930306, trainLoss: 183092.906250, valLoss: 256625.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 29] loss: 164929.111094, trainLoss: 164929.562500, valLoss: 135014.578125\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 30] loss: 180382.283625, trainLoss: 180381.812500, valLoss: 167792.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 31] loss: 154346.518006, trainLoss: 154346.093750, valLoss: 128034.593750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 32] loss: 144850.499991, trainLoss: 144850.859375, valLoss: 153880.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 33] loss: 135403.150813, trainLoss: 135402.578125, valLoss: 115316.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 34] loss: 117905.160378, trainLoss: 117904.187500, valLoss: 102783.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 35] loss: 125444.391775, trainLoss: 125446.039062, valLoss: 115421.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 36] loss: 151859.373791, trainLoss: 151860.921875, valLoss: 116184.179688\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 37] loss: 125125.079531, trainLoss: 125123.789062, valLoss: 124824.367188\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 38] loss: 118117.505775, trainLoss: 118118.015625, valLoss: 100598.609375\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 39] loss: 123366.765406, trainLoss: 123366.179688, valLoss: 114154.710938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 40] loss: 103236.740256, trainLoss: 103236.148438, valLoss: 102107.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 41] loss: 97865.947547, trainLoss: 97866.093750, valLoss: 92873.914062\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 42] loss: 90202.479391, trainLoss: 90203.078125, valLoss: 82050.835938\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 43] loss: 85672.477356, trainLoss: 85674.109375, valLoss: 82018.531250\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 44] loss: 108484.171578, trainLoss: 108483.296875, valLoss: 109560.992188\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.001, epoch = 45] loss: 325814.962009, trainLoss: 325813.468750, valLoss: 196994.687500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 1] loss: 1694842.822600, trainLoss: 1694911.250000, valLoss: 1651516.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 2] loss: 1638235.203600, trainLoss: 1638201.250000, valLoss: 1637633.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 3] loss: 1626177.215800, trainLoss: 1626149.750000, valLoss: 1649658.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 4] loss: 1636204.575400, trainLoss: 1636205.250000, valLoss: 1639561.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 5] loss: 1611186.082800, trainLoss: 1611183.250000, valLoss: 1631127.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 6] loss: 1593068.984600, trainLoss: 1593073.000000, valLoss: 1686602.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 7] loss: 1582488.397600, trainLoss: 1582490.125000, valLoss: 1738836.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 8] loss: 1570263.403400, trainLoss: 1570264.375000, valLoss: 1684311.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 9] loss: 1538749.431800, trainLoss: 1538750.875000, valLoss: 1644571.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 10] loss: 1511667.529500, trainLoss: 1511668.125000, valLoss: 1783399.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 11] loss: 1450697.761500, trainLoss: 1450695.750000, valLoss: 1735045.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 12] loss: 1356124.889800, trainLoss: 1356125.500000, valLoss: 1578738.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 13] loss: 1169875.935000, trainLoss: 1169880.500000, valLoss: 1905932.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 14] loss: 817684.616700, trainLoss: 817686.500000, valLoss: 1337043.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 15] loss: 588282.991600, trainLoss: 588282.125000, valLoss: 990169.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 16] loss: 512863.651575, trainLoss: 512865.625000, valLoss: 1093715.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 17] loss: 561705.358250, trainLoss: 561710.125000, valLoss: 621197.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 18] loss: 524192.985450, trainLoss: 524192.000000, valLoss: 747805.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 19] loss: 527834.106125, trainLoss: 527836.937500, valLoss: 840907.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 20] loss: 492701.016950, trainLoss: 492700.343750, valLoss: 859756.437500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 21] loss: 436150.455675, trainLoss: 436150.000000, valLoss: 744137.375000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 22] loss: 510318.012350, trainLoss: 510315.718750, valLoss: 613193.687500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 23] loss: 493074.838775, trainLoss: 493072.781250, valLoss: 878157.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 24] loss: 422665.934187, trainLoss: 422665.500000, valLoss: 1305951.125000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 25] loss: 392275.815625, trainLoss: 392275.687500, valLoss: 764782.187500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 26] loss: 346650.191375, trainLoss: 346650.968750, valLoss: 1187006.625000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 27] loss: 319005.782012, trainLoss: 319006.406250, valLoss: 634247.562500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 28] loss: 330919.928588, trainLoss: 330922.156250, valLoss: 587921.000000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 29] loss: 340417.269512, trainLoss: 340420.875000, valLoss: 1184757.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 30] loss: 306001.627950, trainLoss: 306003.500000, valLoss: 895002.937500\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 31] loss: 268256.645700, trainLoss: 268256.468750, valLoss: 474821.718750\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 32] loss: 265877.220075, trainLoss: 265878.375000, valLoss: 761493.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 33] loss: 281086.423875, trainLoss: 281089.343750, valLoss: 1133748.500000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 34] loss: 256205.952038, trainLoss: 256205.406250, valLoss: 973510.750000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 35] loss: 250781.947813, trainLoss: 250782.968750, valLoss: 401314.875000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 36] loss: 227092.498588, trainLoss: 227095.078125, valLoss: 637623.250000\n",
      "[batchSize = 64, dropOutProb = 0.5, learningRate = 0.0001, epoch = 37] loss: 241602.222650, trainLoss: 241603.343750, valLoss: 1322341.250000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 1] loss: 350061081.685600, trainLoss: 350060992.000000, valLoss: 1666121.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 2] loss: 1625009.038400, trainLoss: 1625010.750000, valLoss: 1616426.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 3] loss: 1565723.747400, trainLoss: 1565723.000000, valLoss: 1549249.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 4] loss: 1489000.197100, trainLoss: 1489005.375000, valLoss: 1463770.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 5] loss: 1399017.946200, trainLoss: 1399023.125000, valLoss: 1370527.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 6] loss: 1300679.787000, trainLoss: 1300681.750000, valLoss: 1268781.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 7] loss: 1198870.071100, trainLoss: 1198857.875000, valLoss: 1166058.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 8] loss: 1097506.800300, trainLoss: 1097500.500000, valLoss: 1066924.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 9] loss: 999221.494600, trainLoss: 999226.125000, valLoss: 969989.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 10] loss: 905732.364100, trainLoss: 905726.875000, valLoss: 879105.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 11] loss: 817896.114200, trainLoss: 817897.187500, valLoss: 794270.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 12] loss: 736247.227250, trainLoss: 736250.875000, valLoss: 715485.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 13] loss: 661006.634150, trainLoss: 661009.625000, valLoss: 644019.687500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 14] loss: 592276.644150, trainLoss: 592276.937500, valLoss: 578385.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 15] loss: 530112.843850, trainLoss: 530119.125000, valLoss: 519637.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 16] loss: 474383.255650, trainLoss: 474380.593750, valLoss: 466508.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 17] loss: 424958.903550, trainLoss: 424962.531250, valLoss: 419840.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 18] loss: 381719.100775, trainLoss: 381717.843750, valLoss: 380068.656250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 19] loss: 344529.150675, trainLoss: 344529.093750, valLoss: 345296.156250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 20] loss: 313206.844763, trainLoss: 313205.343750, valLoss: 317169.156250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 21] loss: 287449.943763, trainLoss: 287449.000000, valLoss: 293918.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 22] loss: 266966.480812, trainLoss: 266967.375000, valLoss: 275638.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 23] loss: 251328.515850, trainLoss: 251327.687500, valLoss: 262381.062500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 24] loss: 240011.784925, trainLoss: 240009.000000, valLoss: 252966.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 25] loss: 232446.310663, trainLoss: 232447.359375, valLoss: 247032.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 26] loss: 227891.608463, trainLoss: 227892.015625, valLoss: 243618.062500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 27] loss: 225523.568181, trainLoss: 225525.062500, valLoss: 242029.171875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 28] loss: 224489.710925, trainLoss: 224490.156250, valLoss: 241392.921875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 29] loss: 224132.906713, trainLoss: 224131.015625, valLoss: 241205.312500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 30] loss: 224042.065100, trainLoss: 224042.312500, valLoss: 241159.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 31] loss: 224023.249012, trainLoss: 224021.875000, valLoss: 241153.093750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 32] loss: 224023.067225, trainLoss: 224022.265625, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 33] loss: 224020.805037, trainLoss: 224021.171875, valLoss: 241146.187500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 34] loss: 224022.666813, trainLoss: 224023.859375, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 35] loss: 224021.409237, trainLoss: 224023.468750, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 36] loss: 224022.351737, trainLoss: 224021.578125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 37] loss: 224022.312075, trainLoss: 224020.703125, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 38] loss: 224022.999700, trainLoss: 224022.296875, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 39] loss: 224023.595288, trainLoss: 224023.781250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 40] loss: 224021.871700, trainLoss: 224020.890625, valLoss: 241148.640625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 41] loss: 224022.630025, trainLoss: 224023.750000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 42] loss: 224020.417725, trainLoss: 224019.531250, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 43] loss: 224020.350062, trainLoss: 224018.125000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 44] loss: 224021.457525, trainLoss: 224019.828125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 45] loss: 224023.097662, trainLoss: 224022.453125, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 46] loss: 224022.481325, trainLoss: 224023.109375, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 47] loss: 224020.559900, trainLoss: 224019.625000, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 48] loss: 224021.830387, trainLoss: 224022.343750, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 49] loss: 224023.166575, trainLoss: 224022.187500, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 50] loss: 224021.422975, trainLoss: 224021.750000, valLoss: 241146.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 51] loss: 224021.298263, trainLoss: 224021.656250, valLoss: 241145.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.1, epoch = 52] loss: 224022.905212, trainLoss: 224022.656250, valLoss: 241146.187500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 1] loss: 4179061.112150, trainLoss: 4179084.000000, valLoss: 410701.437500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 2] loss: 541705.155387, trainLoss: 541708.187500, valLoss: 419937.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 3] loss: 393563.165725, trainLoss: 393562.500000, valLoss: 401772.343750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 4] loss: 423503.027631, trainLoss: 423506.437500, valLoss: 1575300.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 5] loss: 412562.116787, trainLoss: 412560.937500, valLoss: 388366.875000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 6] loss: 356679.311644, trainLoss: 356684.812500, valLoss: 353623.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 7] loss: 1292179.021112, trainLoss: 1292183.125000, valLoss: 717761.312500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 8] loss: 307963.205619, trainLoss: 307967.875000, valLoss: 149033.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 9] loss: 145550.308831, trainLoss: 145550.375000, valLoss: 132251.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 10] loss: 133788.563362, trainLoss: 133788.218750, valLoss: 125706.046875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 11] loss: 128506.251094, trainLoss: 128506.671875, valLoss: 123580.796875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 12] loss: 125711.969819, trainLoss: 125712.664062, valLoss: 120130.632812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 13] loss: 123588.169913, trainLoss: 123589.679688, valLoss: 120471.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 14] loss: 121946.277475, trainLoss: 121944.226562, valLoss: 117354.164062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 15] loss: 118195.510456, trainLoss: 118196.515625, valLoss: 115575.726562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 16] loss: 117247.815138, trainLoss: 117247.101562, valLoss: 115611.109375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 17] loss: 116186.819525, trainLoss: 116186.351562, valLoss: 109293.796875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 18] loss: 113293.519634, trainLoss: 113292.132812, valLoss: 106929.031250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 19] loss: 111353.799781, trainLoss: 111353.382812, valLoss: 106435.296875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 20] loss: 110269.812050, trainLoss: 110269.617188, valLoss: 108705.523438\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 21] loss: 109843.416569, trainLoss: 109842.867188, valLoss: 111361.132812\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 22] loss: 108115.984319, trainLoss: 108115.296875, valLoss: 106142.710938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 23] loss: 110240.947081, trainLoss: 110239.320312, valLoss: 113850.226562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 24] loss: 107978.365359, trainLoss: 107978.375000, valLoss: 102523.539062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 25] loss: 104769.588475, trainLoss: 104771.070312, valLoss: 99130.601562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 26] loss: 102651.553469, trainLoss: 102651.062500, valLoss: 101446.703125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 27] loss: 104425.270178, trainLoss: 104425.656250, valLoss: 102262.617188\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 28] loss: 103116.497150, trainLoss: 103115.867188, valLoss: 106405.820312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 29] loss: 109777.405522, trainLoss: 109777.406250, valLoss: 106923.687500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 30] loss: 111279.732066, trainLoss: 111278.625000, valLoss: 104402.117188\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 31] loss: 105604.183844, trainLoss: 105604.890625, valLoss: 101235.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.01, epoch = 32] loss: 55197972.700613, trainLoss: 55197952.000000, valLoss: 1594125.375000\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 1] loss: 1680116.619100, trainLoss: 1680106.125000, valLoss: 1317351.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 2] loss: 1129132.999000, trainLoss: 1129148.750000, valLoss: 704212.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 3] loss: 589605.757700, trainLoss: 589603.375000, valLoss: 570615.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 4] loss: 514877.733425, trainLoss: 514867.593750, valLoss: 518733.406250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 5] loss: 463313.076150, trainLoss: 463313.000000, valLoss: 456532.562500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 6] loss: 410020.259187, trainLoss: 410026.218750, valLoss: 413913.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 7] loss: 404972.035700, trainLoss: 404962.500000, valLoss: 424047.031250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 8] loss: 425476.937887, trainLoss: 425477.218750, valLoss: 697926.812500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 9] loss: 366139.820475, trainLoss: 366140.312500, valLoss: 879382.687500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 10] loss: 389372.473263, trainLoss: 389368.187500, valLoss: 375202.343750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 11] loss: 329247.463938, trainLoss: 329248.031250, valLoss: 336119.531250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 12] loss: 310520.099606, trainLoss: 310519.062500, valLoss: 296160.781250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 13] loss: 360538.303600, trainLoss: 360535.375000, valLoss: 317575.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 14] loss: 279283.099962, trainLoss: 279281.156250, valLoss: 327896.562500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 15] loss: 263297.486463, trainLoss: 263299.593750, valLoss: 224656.765625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 16] loss: 1122307.470744, trainLoss: 1122308.125000, valLoss: 376520.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 17] loss: 316371.607219, trainLoss: 316373.625000, valLoss: 278318.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 18] loss: 1909332.957169, trainLoss: 1909335.250000, valLoss: 337428.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 19] loss: 344525.963281, trainLoss: 344523.937500, valLoss: 329936.187500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 20] loss: 314669.094062, trainLoss: 314666.406250, valLoss: 307655.218750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 21] loss: 296030.212487, trainLoss: 296032.750000, valLoss: 289991.968750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 22] loss: 271996.563281, trainLoss: 272001.531250, valLoss: 248245.515625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 23] loss: 240030.513231, trainLoss: 240030.406250, valLoss: 223517.953125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 24] loss: 258592.425919, trainLoss: 258591.765625, valLoss: 233470.328125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 25] loss: 284423.889131, trainLoss: 284425.562500, valLoss: 254956.078125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 26] loss: 246206.325809, trainLoss: 246204.593750, valLoss: 227261.609375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 27] loss: 220556.451366, trainLoss: 220554.562500, valLoss: 205664.062500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 28] loss: 207668.457069, trainLoss: 207665.390625, valLoss: 185762.562500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 29] loss: 192474.859409, trainLoss: 192472.781250, valLoss: 176451.359375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 30] loss: 225543.403187, trainLoss: 225542.875000, valLoss: 186127.265625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 31] loss: 200631.117300, trainLoss: 200632.718750, valLoss: 165256.718750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 32] loss: 174096.042106, trainLoss: 174096.578125, valLoss: 165253.140625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 33] loss: 316418.057753, trainLoss: 316419.718750, valLoss: 245616.203125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 34] loss: 312896.573237, trainLoss: 312897.437500, valLoss: 163427.406250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 35] loss: 168445.501100, trainLoss: 168445.843750, valLoss: 138215.031250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 36] loss: 160615.351822, trainLoss: 160615.843750, valLoss: 131616.593750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 37] loss: 142961.490778, trainLoss: 142960.250000, valLoss: 114660.070312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 38] loss: 130826.926600, trainLoss: 130826.375000, valLoss: 105558.039062\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 39] loss: 122288.577447, trainLoss: 122289.164062, valLoss: 121671.445312\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 40] loss: 237780.990903, trainLoss: 237783.156250, valLoss: 236681.312500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 41] loss: 203533.251778, trainLoss: 203531.406250, valLoss: 178559.140625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 42] loss: 168029.007159, trainLoss: 168031.234375, valLoss: 144127.062500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 43] loss: 148340.928038, trainLoss: 148341.125000, valLoss: 112967.468750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 44] loss: 457673.613197, trainLoss: 457669.562500, valLoss: 162658.953125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 45] loss: 162855.212984, trainLoss: 162854.437500, valLoss: 172323.203125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 46] loss: 171933.576812, trainLoss: 171936.156250, valLoss: 153159.515625\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 47] loss: 158596.720347, trainLoss: 158594.593750, valLoss: 127072.351562\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 48] loss: 227872.228341, trainLoss: 227874.812500, valLoss: 171794.984375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 49] loss: 195117.558322, trainLoss: 195120.281250, valLoss: 168316.359375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 50] loss: 190498.370184, trainLoss: 190497.062500, valLoss: 145650.703125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 51] loss: 149478.713831, trainLoss: 149476.531250, valLoss: 115465.085938\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 52] loss: 139605.473022, trainLoss: 139604.140625, valLoss: 107848.046875\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 53] loss: 179733.309541, trainLoss: 179734.234375, valLoss: 108213.734375\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 54] loss: 168139.189100, trainLoss: 168136.562500, valLoss: 135503.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 55] loss: 134169.957569, trainLoss: 134170.484375, valLoss: 93157.078125\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 56] loss: 124700.846800, trainLoss: 124700.859375, valLoss: 90923.679688\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.001, epoch = 57] loss: 714705.458866, trainLoss: 714705.437500, valLoss: 180247.406250\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 1] loss: 1714129.064600, trainLoss: 1714163.625000, valLoss: 1656840.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 2] loss: 1646812.580600, trainLoss: 1647016.625000, valLoss: 1657040.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 3] loss: 1642901.015200, trainLoss: 1642940.500000, valLoss: 1671863.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 4] loss: 1642559.998400, trainLoss: 1642421.125000, valLoss: 1677258.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 5] loss: 1627706.759000, trainLoss: 1627590.750000, valLoss: 1663331.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 6] loss: 1631720.630800, trainLoss: 1631741.000000, valLoss: 1663649.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 7] loss: 1619528.340400, trainLoss: 1619465.375000, valLoss: 1689734.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 8] loss: 1613788.361200, trainLoss: 1613791.750000, valLoss: 1719362.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 9] loss: 1606733.637200, trainLoss: 1606743.250000, valLoss: 1694884.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 10] loss: 1595408.050200, trainLoss: 1595405.625000, valLoss: 1724435.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 11] loss: 1599003.792800, trainLoss: 1599007.250000, valLoss: 1714704.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 12] loss: 1584301.244400, trainLoss: 1584302.625000, valLoss: 1757393.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 13] loss: 1578612.304800, trainLoss: 1578614.500000, valLoss: 1727151.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 14] loss: 1561272.126600, trainLoss: 1561272.125000, valLoss: 1708104.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 15] loss: 1549921.467600, trainLoss: 1549919.750000, valLoss: 1798842.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 16] loss: 1536898.047600, trainLoss: 1536903.375000, valLoss: 1753738.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 17] loss: 1509765.069600, trainLoss: 1509767.625000, valLoss: 1774944.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 18] loss: 1478973.786300, trainLoss: 1478978.875000, valLoss: 1785075.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 19] loss: 1422854.674200, trainLoss: 1422853.500000, valLoss: 1649861.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 20] loss: 1363189.072300, trainLoss: 1363191.500000, valLoss: 1654831.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 21] loss: 1259492.675100, trainLoss: 1259490.375000, valLoss: 1493734.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 22] loss: 1068553.805000, trainLoss: 1068558.875000, valLoss: 1202609.375000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 23] loss: 834921.737050, trainLoss: 834925.500000, valLoss: 977519.875000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 24] loss: 596499.649150, trainLoss: 596497.937500, valLoss: 1105540.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 25] loss: 476989.950875, trainLoss: 476989.187500, valLoss: 958251.625000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 26] loss: 466481.465100, trainLoss: 466481.750000, valLoss: 621561.500000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 27] loss: 447945.685425, trainLoss: 447946.125000, valLoss: 571657.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 28] loss: 449298.445575, trainLoss: 449302.625000, valLoss: 892540.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 29] loss: 442600.061412, trainLoss: 442603.562500, valLoss: 534769.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 30] loss: 409313.255600, trainLoss: 409310.812500, valLoss: 681765.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 31] loss: 410640.409275, trainLoss: 410642.218750, valLoss: 647627.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 32] loss: 398185.235325, trainLoss: 398183.937500, valLoss: 514230.062500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 33] loss: 406293.198463, trainLoss: 406293.375000, valLoss: 576103.562500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 34] loss: 392743.166113, trainLoss: 392741.687500, valLoss: 510857.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 35] loss: 421495.456450, trainLoss: 421496.093750, valLoss: 538364.250000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 36] loss: 394697.083500, trainLoss: 394696.437500, valLoss: 664039.000000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 37] loss: 362961.529913, trainLoss: 362963.281250, valLoss: 797667.937500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 38] loss: 338385.569875, trainLoss: 338388.218750, valLoss: 851862.125000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 39] loss: 346201.641813, trainLoss: 346201.281250, valLoss: 726540.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 40] loss: 342417.845712, trainLoss: 342416.437500, valLoss: 420327.281250\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 41] loss: 313511.503925, trainLoss: 313510.968750, valLoss: 485509.093750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 42] loss: 316491.909237, trainLoss: 316493.250000, valLoss: 524927.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 43] loss: 309411.387825, trainLoss: 309409.468750, valLoss: 496220.593750\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 44] loss: 319946.815687, trainLoss: 319945.937500, valLoss: 549673.812500\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 45] loss: 295914.016063, trainLoss: 295916.437500, valLoss: 602879.750000\n",
      "[batchSize = 64, dropOutProb = 0.7, learningRate = 0.0001, epoch = 46] loss: 291534.420600, trainLoss: 291534.062500, valLoss: 665180.687500\n",
      "Validation loss starting to increase. Exiting...\n",
      "Finished training...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convergence params\n",
    "movingWindowSize = 20\n",
    "\n",
    "# multiprocessing cpu params\n",
    "numWorkers = 4\n",
    "\n",
    "# define dataset variables to adapt to dataloader format\n",
    "\n",
    "# training set. divide into batches inside the training loop\n",
    "trainingListIDs = df_train.index.get_level_values('trace_name').tolist()\n",
    "trainingLabels = (df_train['s_arrival_sample']).to_dict()\n",
    "\n",
    "\n",
    "# validation set\n",
    "validationListIDs = df_val.index.get_level_values('trace_name').tolist()\n",
    "validationLabels = (df_val['s_arrival_sample']).to_dict()\n",
    "\n",
    "validationSet = Dataset(validationListIDs, validationLabels)\n",
    "validationGenerator = torch.utils.data.DataLoader(validationSet, batch_size = 256, num_workers = numWorkers)\n",
    "\n",
    "# test set\n",
    "testListIDs = df_test.index.get_level_values('trace_name').tolist()\n",
    "testLabels = (df_test['s_arrival_sample']).to_dict()\n",
    "\n",
    "testSet = Dataset(testListIDs, testLabels)\n",
    "testGenerator = torch.utils.data.DataLoader(testSet, batch_size = 256, num_workers = numWorkers)\n",
    "    \n",
    "# hyperParameters\n",
    "numEpochs = 200\n",
    "batchSize = 64\n",
    "learningRateList = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "dropOutProbList = [0.3, 0.5, 0.7]\n",
    "#learningRateList = [1e-3]\n",
    "#dropOutProbList = [0.5]\n",
    "\n",
    "# training\n",
    "lossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "lossProgression[:] = np.nan\n",
    "\n",
    "trainLossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "trainLossProgression[:] = np.nan\n",
    "\n",
    "valLossProgression = np.zeros((len(dropOutProbList), len(learningRateList), numEpochs))\n",
    "valLossProgression[:] = np.nan\n",
    "\n",
    "bestValLoss = np.zeros((len(dropOutProbList), len(learningRateList)))\n",
    "bestValLoss[:] = np.inf\n",
    "\n",
    "\n",
    "for d,dropOutProb in enumerate (dropOutProbList):\n",
    "    \n",
    "    numBatches = int(len(df_train)/batchSize) + 1 if len(df_train) % batchSize != 0 else int(len(df_train)/batchSize)\n",
    "    \n",
    "    # invoke dataloader with appropriate batchSize\n",
    "    trainingSet = Dataset(trainingListIDs, trainingLabels)\n",
    "    trainingGenerator = torch.utils.data.DataLoader(trainingSet, batch_size = batchSize, shuffle=True, num_workers = numWorkers)        \n",
    "    \n",
    "    for l,learningRate in enumerate(learningRateList):\n",
    "        \n",
    "        # create a neuralNet object\n",
    "        net = Net(dropOutProb)        \n",
    "        net.to(device)\n",
    "        \n",
    "        \n",
    "        # optimizer type\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learningRate)\n",
    "        \n",
    "        # movingWindow of previous epochs to check for convergence\n",
    "        deltaValLossHistory = []\n",
    "        prevValLoss = np.inf        \n",
    "        \n",
    "        fileTag = f'batchSize_{batchSize}_learningRate_{learningRate}_dropOutProb_{dropOutProb}_numEpochs_{numEpochs}_dataFraction_{earthquakeSampleFraction}_{noiseSampleFraction}'\n",
    "        \n",
    "        for epoch in range(numEpochs):                        \n",
    "            \n",
    "            # set network in training mode\n",
    "            net.train()\n",
    "            \n",
    "            runningLoss = 0.0\n",
    "            sqError = 0.0\n",
    "            for i, data in enumerate(trainingGenerator, 0):\n",
    "\n",
    "                inputBatch, groundTruthBatch = data\n",
    "                groundTruthBatch = torch.tensor(groundTruthBatch, dtype = torch.float32)\n",
    "                inputBatch, groundTruthBatch = inputBatch.to(device), groundTruthBatch.to(device)\n",
    "                                                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputPredBatch = net(inputBatch)\n",
    "                \n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(outputPredBatch,groundTruthBatch.unsqueeze(1))\n",
    "                \n",
    "                # backprop\n",
    "                loss.backward()\n",
    "                \n",
    "                # gradient descent step\n",
    "                optimizer.step()    \n",
    "                \n",
    "                # output prediction label\n",
    "                outputPredLabelBatch = torch.round(outputPredBatch)\n",
    "\n",
    "                # collect training loss, accuracy statistics\n",
    "                if i == numBatches - 1:\n",
    "                    runningLoss += loss.item() * (len(df_train) - i*batchSize)                    \n",
    "                else:                        \n",
    "                    runningLoss += loss.item() * batchSize\n",
    "                \n",
    "                sqError += torch.sum((outputPredLabelBatch.squeeze() - groundTruthBatch)**2)\n",
    "            \n",
    "            # track training loss/accuracy Progression\n",
    "            avgLoss = runningLoss/len(df_train)\n",
    "            lossProgression[d, l, epoch] = avgLoss\n",
    "            trainLoss = sqError/len(df_train)\n",
    "            trainLossProgression[d,l, epoch] = trainLoss\n",
    "                        \n",
    "            # evaluate validation set accuracy\n",
    "            \n",
    "            # set network in eval mode\n",
    "            net.eval()\n",
    "            \n",
    "            sqError = 0.0\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for i, data in enumerate(validationGenerator, 0):\n",
    "\n",
    "                    inputBatch, groundTruthBatch = data\n",
    "                    groundTruthBatch = torch.tensor(groundTruthBatch, dtype = torch.float32)\n",
    "                    inputBatch, groundTruthBatch = inputBatch.to(device), groundTruthBatch.to(device)\n",
    "                    outputPredBatch = net(inputBatch)\n",
    "                    \n",
    "                    # output prediction label\n",
    "                    outputPredLabelBatch = torch.round(outputPredBatch)\n",
    "                    \n",
    "                    sqError += torch.sum((outputPredLabelBatch.squeeze() - groundTruthBatch)**2)\n",
    "\n",
    "            valLoss = sqError/len(df_val)\n",
    "            valLossProgression[d,l,epoch] = valLoss\n",
    "                        \n",
    "            print('[batchSize = %d, dropOutProb = %.2g, learningRate = %.2g, epoch = %d] loss: %f, trainLoss: %f, valLoss: %f' %\n",
    "                                                (batchSize, dropOutProb, learningRate, epoch + 1, avgLoss, trainLoss, valLoss))\n",
    "            \n",
    "            # update network parameters if validation accuracy in current epoch is better than the past            \n",
    "            if valLoss < bestValLoss[d,l]:\n",
    "                \n",
    "                # store the neuralNet parameters in a file                \n",
    "                path = f'{netParamsDirName}/cnn3Layer_12_12_8_fc2layer_16_16_{fileTag}.pth'\n",
    "                torch.save(net.state_dict(), path)\n",
    "                \n",
    "                bestValLoss[d,l] = valLoss\n",
    "            \n",
    "            # check for convergence and exit early if valLoss is not improving\n",
    "            deltaValLoss = valLoss - prevValLoss\n",
    "            deltaValLossHistory.insert(0, deltaValLoss)            \n",
    "            \n",
    "            if epoch >= movingWindowSize:\n",
    "                deltaValLossHistory.pop()\n",
    "                \n",
    "                if sum(deltaValLossHistory) >= 0:\n",
    "                    print ('Validation loss starting to increase. Exiting...')\n",
    "                    break\n",
    "                \n",
    "            prevValLoss = valLoss                    \n",
    "        \n",
    "        print ('Finished training...')\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
